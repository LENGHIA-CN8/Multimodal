{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "_WijfxppfYfk",
    "outputId": "59217166-a6fa-4270-d536-d0aec5224098"
   },
   "outputs": [],
   "source": [
    "# !pip install pytorch-lightning\n",
    "# !pip install transformers\n",
    "# !pip install vncorenlp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "YEE6ZUhdK8DJ",
    "outputId": "6f3c47ea-f836-4cee-e288-3b0492153fa2"
   },
   "outputs": [],
   "source": [
    "# from google.colab import drive\n",
    "# drive.mount(\"/content/drive\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wed Jun 29 17:17:07 2022       \n",
      "+-----------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 465.19.01    Driver Version: 465.19.01    CUDA Version: 11.3     |\n",
      "|-------------------------------+----------------------+----------------------+\n",
      "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
      "|                               |                      |               MIG M. |\n",
      "|===============================+======================+======================|\n",
      "|   0  NVIDIA GeForce ...  Off  | 00000000:5E:00.0 Off |                  N/A |\n",
      "| 29%   53C    P2    48W / 250W |   3294MiB / 11019MiB |     21%      Default |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "|   1  NVIDIA GeForce ...  Off  | 00000000:86:00.0 Off |                  N/A |\n",
      "|  0%   24C    P8     3W / 250W |   4518MiB / 11019MiB |      0%      Default |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "                                                                               \n",
      "+-----------------------------------------------------------------------------+\n",
      "| Processes:                                                                  |\n",
      "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
      "|        ID   ID                                                   Usage      |\n",
      "|=============================================================================|\n",
      "|    0   N/A  N/A   3735335      C   ...da3/envs/AI_DA/bin/python     3291MiB |\n",
      "|    1   N/A  N/A   2483339      C   python                           4515MiB |\n",
      "+-----------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !kill -9 3980682"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "-r5rwk7EG8Et"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/nghiatl/anaconda3/envs/AI_DA/lib/python3.10/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch \n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader, random_split\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import zipfile\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "import re\n",
    "import pytorch_lightning as pl\n",
    "from torchvision import transforms as T\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "train_ratio = 0.8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "# SEED_VALUE = random.randint(0,100)\n",
    "SEED_VALUE = 0\n",
    "# print(SEED_VALUE)\n",
    "random.seed(SEED_VALUE)\n",
    "torch.manual_seed(SEED_VALUE)\n",
    "torch.cuda.manual_seed(SEED_VALUE)\n",
    "np.random.seed(SEED_VALUE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "5JN5uIr4LBuS",
    "outputId": "5549bc0c-f90d-47dc-e4ea-2ace1f478985"
   },
   "outputs": [],
   "source": [
    "# %cd /content/drive/MyDrive/Colab Notebooks/Multimodal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "QOPX9jdjgwaB",
    "outputId": "fce64c1f-5b21-40b5-e362-57f5b14037a2"
   },
   "outputs": [],
   "source": [
    "# !ls\n",
    "data_zip_file = './local1_folder.zip'\n",
    "bin_zip_file = './mountains.zip'\n",
    "\n",
    "csv_file2 = './OUTPUT/Comment_22.csv'\n",
    "csv_file3 = './OUTPUT/Comment_23.csv'\n",
    "csv_file4 = './OUTPUT/Comment_24.csv'\n",
    "csv_file5 = './OUTPUT/Comment_25.csv'\n",
    "\n",
    "csv_file = './OUTPUT/Comment.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wed Jun 29 17:17:21 2022       \n",
      "+-----------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 465.19.01    Driver Version: 465.19.01    CUDA Version: 11.3     |\n",
      "|-------------------------------+----------------------+----------------------+\n",
      "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
      "|                               |                      |               MIG M. |\n",
      "|===============================+======================+======================|\n",
      "|   0  NVIDIA GeForce ...  Off  | 00000000:5E:00.0 Off |                  N/A |\n",
      "|  0%   54C    P2    46W / 250W |   3294MiB / 11019MiB |     21%      Default |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "|   1  NVIDIA GeForce ...  Off  | 00000000:86:00.0 Off |                  N/A |\n",
      "|  0%   24C    P8     2W / 250W |   4518MiB / 11019MiB |      0%      Default |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "                                                                               \n",
      "+-----------------------------------------------------------------------------+\n",
      "| Processes:                                                                  |\n",
      "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
      "|        ID   ID                                                   Usage      |\n",
      "|=============================================================================|\n",
      "|    0   N/A  N/A   3735335      C   ...da3/envs/AI_DA/bin/python     3291MiB |\n",
      "|    1   N/A  N/A   2483339      C   python                           4515MiB |\n",
      "+-----------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YinE6ts__dfB"
   },
   "source": [
    "# Data preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HwSAFGjcLttP"
   },
   "source": [
    "## Image data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "AuYQmzhSgxiW"
   },
   "outputs": [],
   "source": [
    "data_zip = zipfile.ZipFile(data_zip_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: './mountains.zip'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Input \u001b[0;32mIn [12]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0m bin_zip \u001b[38;5;241m=\u001b[39m \u001b[43mzipfile\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mZipFile\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbin_zip_file\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/AI_DA/lib/python3.10/zipfile.py:1240\u001b[0m, in \u001b[0;36mZipFile.__init__\u001b[0;34m(self, file, mode, compression, allowZip64, compresslevel, strict_timestamps)\u001b[0m\n\u001b[1;32m   1238\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[1;32m   1239\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 1240\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfp \u001b[38;5;241m=\u001b[39m \u001b[43mio\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mopen\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfile\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfilemode\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1241\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mOSError\u001b[39;00m:\n\u001b[1;32m   1242\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m filemode \u001b[38;5;129;01min\u001b[39;00m modeDict:\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: './mountains.zip'"
     ]
    }
   ],
   "source": [
    "bin_zip = zipfile.ZipFile(bin_zip_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(bin_zip.namelist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'mountains/mountains_000581.png' in bin_zip.namelist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "5EJSSyN5UEqZ",
    "outputId": "3f225eae-f16d-4305-c2dc-2a83376642b8"
   },
   "outputs": [],
   "source": [
    "len(data_zip.namelist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'local1_folder-1/foody-fukunohana-hanoi-japanese-retstaurant-970-637197316556573205.jpg' in data_zip.namelist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 340
    },
    "id": "DdGQBwnJg_Bs",
    "outputId": "19086978-ed5b-4920-d305-7728a4eff1cb"
   },
   "outputs": [],
   "source": [
    "img_path = data_zip.namelist()[1]\n",
    "print(img_path)\n",
    "img = Image.open(data_zip.open(img_path))\n",
    "print(img)\n",
    "print(np.asarray(img).shape)\n",
    "plt.imshow(img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# img_path = bin_zip.namelist()[13]\n",
    "img_path = 'mountains/mountains_000122.png'\n",
    "\n",
    "print(img_path)\n",
    "img = Image.open(bin_zip.open(img_path))\n",
    "print(type(img))\n",
    "print(img)\n",
    "print(np.asarray(img).shape[2])\n",
    "plt.imshow(img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "LD9uOa1eiRj7"
   },
   "outputs": [],
   "source": [
    "# ### Scale image to [0,1]\n",
    "# trans = T.ToTensor()\n",
    "# trans(img).shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NMRLRh98LpkS"
   },
   "source": [
    "## CSV data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 206
    },
    "id": "Pg2j5eX7kJ2I",
    "outputId": "688189ef-369e-4559-b4f8-a2a2093f3c17"
   },
   "outputs": [],
   "source": [
    "data_df1 = pd.read_csv(csv_file)\n",
    "data_df2 = pd.read_csv(csv_file2)\n",
    "data_df3 = pd.read_csv(csv_file3)\n",
    "data_df4 = pd.read_csv(csv_file4)\n",
    "data_df5 = pd.read_csv(csv_file5)\n",
    "\n",
    "data_df1[['Comment', 'Rating']].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "aZ4rpv-1tdNu",
    "outputId": "1cfdc147-fb29-4a68-e65c-17551869e713"
   },
   "outputs": [],
   "source": [
    "data_df4.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_df4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def insert_bin_image(df):\n",
    "#     sample_df = df.sample(frac=0.05, random_state=0)\n",
    "#     df.drop(sample_df.index, inplace=True)\n",
    "#     df = df.reset_index(drop=True)\n",
    "#     sample_df = sample_df.reset_index(drop=True)\n",
    "#     for i in range(len(sample_df)):\n",
    "#         list_img = []\n",
    "#         for j in range(len(sample_df.iloc[i]['image_urls'].split(','))):\n",
    "#             num_ran = random.randint(0, 900)\n",
    "#             num_img = '0'* (6-len(str(num_ran))) + str(num_ran)\n",
    "# #             mountains/mountains_000580.png\n",
    "#             list_img.append('mountains/mountains_'+ num_img + '.png')\n",
    "#         sample_df.loc[i,'image_urls'] = ','.join(list_img)\n",
    "    \n",
    "#     df = pd.concat([df, sample_df], ignore_index = True)\n",
    "    \n",
    "#     return df\n",
    "\n",
    "# data_df4 = insert_bin_image(data_df4)\n",
    "# data_df4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-oQbCbM4gkOH"
   },
   "outputs": [],
   "source": [
    "data_df = pd.concat([data_df1,data_df2,data_df3,data_df4, data_df5], ignore_index = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "XHC1UlnMtlcz",
    "outputId": "dfe5d7e3-91a6-44f9-a944-4a3e6366f520"
   },
   "outputs": [],
   "source": [
    "data_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "DFR0Di-d-KCX"
   },
   "outputs": [],
   "source": [
    "data_df = data_df.dropna()\n",
    "data_df = data_df.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "sVhY9Oj9iwsf",
    "outputId": "d02fc229-a419-44b9-a985-1e9f36cb6d07"
   },
   "outputs": [],
   "source": [
    "data_df.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 773
    },
    "id": "uTTgx8nBJaIf",
    "outputId": "4febeb01-8ff8-4dc5-cb3d-330416a3154c"
   },
   "outputs": [],
   "source": [
    "## Add more data\n",
    "data_df.rename(columns={\"Rating\":\"score\"}, inplace=True)\n",
    "data_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 53
    },
    "id": "m87U0yoinm9x",
    "outputId": "c15d55ad-880d-4e67-89d4-25fb2a849ce1"
   },
   "outputs": [],
   "source": [
    "data_df['image_urls'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 423
    },
    "id": "pPmcItkiypTV",
    "outputId": "5f3bfcb3-8015-48e0-9b16-8231d5921b2e"
   },
   "outputs": [],
   "source": [
    "# ## Calculate score\n",
    "# data_df['score'] = data_df.apply(lambda x: (x['Food_score_cmt']+x['Services_score_cmt']+x['Atmosphere_score_cmt']+x['Position_score_cmt']+x['Price_score_cmt'])/5, axis = 1)\n",
    "data_df[(data_df['score'] >= 8) & (data_df['score'] <= 8)][['Comment','score']].sort_values(by=['score'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 89
    },
    "id": "Vw82otAuoXKO",
    "outputId": "09ab683f-f722-440f-d9e2-74bf27527e34"
   },
   "outputs": [],
   "source": [
    "data_df['Comment'][8]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "2vytA2UJB4e5"
   },
   "outputs": [],
   "source": [
    "## label data\n",
    "def label_comment(x):\n",
    "  if x['score'] >= 8:\n",
    "    return 1\n",
    "  elif (x['score'] >= 6) & (x['score'] < 8):\n",
    "    return 2\n",
    "  else:\n",
    "    return 0\n",
    "data_df['label'] = data_df.apply(label_comment, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'data_df' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[0;32mIn [13]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0m data_df \u001b[38;5;241m=\u001b[39m \u001b[43mdata_df\u001b[49m[data_df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlabel\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m!=\u001b[39m \u001b[38;5;241m2\u001b[39m]\n",
      "\u001b[0;31mNameError\u001b[0m: name 'data_df' is not defined"
     ]
    }
   ],
   "source": [
    "data_df = data_df[data_df['label'] != 2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 354
    },
    "id": "Oym9ogwW97m1",
    "outputId": "9b2b8026-7d6b-4e5c-c569-90af870cd086"
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'data_df' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[0;32mIn [14]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[43mdata_df\u001b[49m[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlabel\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mvalue_counts())\n\u001b[1;32m      2\u001b[0m data_df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlabel\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mvalue_counts()\u001b[38;5;241m.\u001b[39mplot(kind\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbarh\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'data_df' is not defined"
     ]
    }
   ],
   "source": [
    "print(data_df['label'].value_counts())\n",
    "data_df['label'].value_counts().plot(kind='barh')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 206
    },
    "id": "J8CW9ISc6o4G",
    "outputId": "daa2c6d8-639e-42c9-e76f-ef92125c48ca"
   },
   "outputs": [],
   "source": [
    "data_df[['Comment','label', 'score']].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 423
    },
    "id": "5BGOclJx6q07",
    "outputId": "48837a3b-9c51-4c4e-9a7f-87d4ab42d742"
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'data_df' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[0;32mIn [15]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0m data_df \u001b[38;5;241m=\u001b[39m \u001b[43mdata_df\u001b[49m\u001b[38;5;241m.\u001b[39mdrop([\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mFood_score_cmt\u001b[39m\u001b[38;5;124m'\u001b[39m,\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mServices_score_cmt\u001b[39m\u001b[38;5;124m'\u001b[39m,\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mAtmosphere_score_cmt\u001b[39m\u001b[38;5;124m'\u001b[39m,\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mPosition_score_cmt\u001b[39m\u001b[38;5;124m'\u001b[39m,\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mPrice_score_cmt\u001b[39m\u001b[38;5;124m'\u001b[39m],axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m      2\u001b[0m data_df\u001b[38;5;241m.\u001b[39mhead()\n",
      "\u001b[0;31mNameError\u001b[0m: name 'data_df' is not defined"
     ]
    }
   ],
   "source": [
    "data_df = data_df.drop(['Food_score_cmt','Services_score_cmt','Atmosphere_score_cmt','Position_score_cmt','Price_score_cmt'],axis=1)\n",
    "data_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_imgpath(df):\n",
    "    img_p = []\n",
    "    for path in df['image_urls'].split(\",\"):\n",
    "        img_path = 'local1_folder-1/' + path.split('/')[-1]\n",
    "        if img_path in data_zip.namelist():\n",
    "            img_p.append(img_path)\n",
    "    if len(img_p) > 0:\n",
    "        return ','.join(img_p)\n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_df['image_urls'] = data_df.apply(preprocess_imgpath, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RevId         0\n",
       "UserId        0\n",
       "ResId         0\n",
       "Comment       0\n",
       "image_urls    0\n",
       "score         0\n",
       "label         0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_df = data_df.dropna()\n",
    "data_df = data_df.reset_index(drop=True)\n",
    "data_df.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 354
    },
    "id": "KPh6mSQK-pDj",
    "outputId": "115478a6-9a8e-4d84-e18c-5c8dddafbc0a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1    6952\n",
      "0    3023\n",
      "Name: label, dtype: int64\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<AxesSubplot:>"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAWoAAAD4CAYAAADFAawfAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjUuMSwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/YYfK9AAAACXBIWXMAAAsTAAALEwEAmpwYAAAKdElEQVR4nO3dX6hl51nH8d/jTCc1aZkknVBCEnoSCJFApQ1DbbAUUYz5I/WmFxMEg1YC/gGLFzKhIHgXvRAVim3QqBeaVuu/kFZibQsFkbQnbdJMGsdM60gmNB2rdCr2QhtfL/Z7kp1xxhnj3nuek/l84HDWWnvnPc+ZrPmevdfeJ6kxRgDo67su9AAA/O+EGqA5oQZoTqgBmhNqgOb2rmPRAwcOjK2trXUsDfCa9Pjjj39jjHHVmW5bS6i3trayvb29jqUBXpOq6p/OdptLHwDNCTVAc0IN0JxQAzQn1ADNCTVAc0IN0JxQAzQn1ADNCTVAc0IN0JxQAzQn1ADNCTVAc0IN0JxQAzQn1ADNCTVAc0IN0JxQAzQn1ADNCTVAc0IN0JxQAzQn1ADNCTVAc3vXsehTz5/K1uGPr2NpXkOO33/XhR4BdgWPqAGaE2qA5oQaoDmhBmhOqAGaE2qA5oQaoDmhBmhOqAGaE2qA5oQaoDmhBmhOqAGaE2qA5oQaoDmhBmhOqAGaE2qA5oQaoDmhBmhOqAGaE2qA5oQaoLnzCnVV3V5VR6vqWFUdXvdQALzsnKGuqj1JPpjkjiQ3J7m7qm5e92AALJzPI+p3JDk2xvjqGOM/knwkyY+tdywAdpxPqK9J8tzS/ol57BWq6t6q2q6q7Re/fWpV8wFc9Fb2YuIY44ExxsExxsE9l+5f1bIAF73zCfXzSa5b2r92HgNgA84n1J9PcmNVXV9V+5IcSvLwescCYMfec91hjPGdqvr5JI8m2ZPkwTHG02ufDIAk5xHqJBljfCLJJ9Y8CwBn4DcTAZoTaoDmhBqgOaEGaE6oAZoTaoDmhBqgOaEGaE6oAZoTaoDmhBqgOaEGaE6oAZoTaoDmhBqgOaEGaE6oAZoTaoDmhBqgOaEGaE6oAZo7r/8L+f/VW6/Zn+3771rH0gAXHY+oAZoTaoDmhBqgOaEGaE6oAZoTaoDmhBqgOaEGaE6oAZoTaoDmhBqgOaEGaE6oAZoTaoDmhBqgOaEGaE6oAZoTaoDmhBqgOaEGaE6oAZoTaoDmhBqgOaEGaE6oAZoTaoDmhBqgOaEGaE6oAZoTaoDmhBqgOaEGaE6oAZoTaoDmhBqgOaEGaE6oAZoTaoDmhBqgOaEGaE6oAZoTaoDmhBqgOaEGaE6oAZoTaoDmhBqgOaEGaE6oAZoTaoDmhBqgOaEGaE6oAZrbu45Fn3r+VLYOf3wdSwO0dPz+u9a2tkfUAM0JNUBzQg3QnFADNCfUAM0JNUBzQg3QnFADNCfUAM0JNUBzQg3QnFADNCfUAM0JNUBzQg3QnFADNCfUAM0JNUBzQg3QnFADNCfUAM0JNUBzQg3Q3DlDXVUPVtXJqjqyiYEAeKXzeUT9+0luX/McAJzFOUM9xvhskn/dwCwAnMHKrlFX1b1VtV1V2y9++9SqlgW46K0s1GOMB8YYB8cYB/dcun9VywJc9LzrA6A5oQZo7nzenvdQkr9LclNVnaiq961/LAB27D3XHcYYd29iEADOzKUPgOaEGqA5oQZoTqgBmhNqgOaEGqA5oQZoTqgBmhNqgOaEGqA5oQZoTqgBmhNqgOaEGqA5oQZoTqgBmhNqgOaEGqA5oQZoTqgBmhNqgOaEGqC5vetY9K3X7M/2/XetY2mAi45H1ADNCTVAc0IN0JxQAzQn1ADNCTVAc0IN0JxQAzQn1ADNCTVAc0IN0JxQAzQn1ADNCTVAc0IN0JxQAzQn1ADNCTVAc0IN0JxQAzQn1ADNCTVAc0IN0JxQAzQn1ADNCTVAczXGWP2iVf+W5OjKF16vA0m+caGHeBXMvVnm3qyLae63jDGuOtMNe///85zR0THGwTWtvRZVtb3bZk7MvWnm3ixzL7j0AdCcUAM0t65QP7CmdddpN86cmHvTzL1Z5s6aXkwEYHVc+gBoTqgBmltpqKvq9qo6WlXHqurwKtd+lfM8WFUnq+rI0rErq+qTVfXs/HzFPF5V9Vtz9i9V1S1L/8w98/7PVtU9a575uqr6TFV9uaqerqpf2CVzv76qPldVT865f2Uev76qHpvzfbSq9s3jl8z9Y/P2raW17pvHj1bVj6xz7qWvuaeqvlhVj+yWuavqeFU9VVVPVNX2PNb6PJlf7/Kq+lhV/X1VPVNVt3afu6pumn/OOx/fqqr3b2zuMcZKPpLsSfKVJDck2ZfkySQ3r2r9VznTu5PckuTI0rFfS3J4bh9O8qtz+84kf5WkkrwzyWPz+JVJvjo/XzG3r1jjzFcnuWVuvzHJPyS5eRfMXUneMLdfl+SxOc8fJzk0j38oyc/M7Z9N8qG5fSjJR+f2zfPcuSTJ9fOc2rOBc+UXk/xRkkfmfvu5kxxPcuC0Y63Pk/k1/yDJT8/tfUku3w1zL82/J8kLSd6yqblXOfytSR5d2r8vyX2b+IM7x1xbeWWojya5em5fncUv5yTJh5Pcffr9ktyd5MNLx19xvw3M/5dJfng3zZ3k0iRfSPJ9Wfx21t7Tz5Ekjya5dW7vnfer08+b5futcd5rk3wqyQ8meWTOsRvmPp7/GerW50mS/Un+MfONDLtl7tNmvS3J325y7lVe+rgmyXNL+yfmsW7ePMb42tx+Icmb5/bZ5r9g39d8Wv32LB6dtp97Xj54IsnJJJ/M4lHlN8cY3znDDC/NN28/leRNF2LuJL+R5JeS/Nfcf1N2x9wjyV9X1eNVde881v08uT7JPyf5vXmp6Xeq6rJdMPeyQ0kemtsbmfuifjFxLH6ktXx/YlW9IcmfJnn/GONby7d1nXuM8eIY421ZPEJ9R5LvubATnVtV/WiSk2OMxy/0LK/Cu8YYtyS5I8nPVdW7l29sep7szeJy5G+PMd6e5N+zuGTwkqZzJ0nmaxXvSfInp9+2zrlXGernk1y3tH/tPNbN16vq6iSZn0/O42ebf+PfV1W9LotI/+EY4892y9w7xhjfTPKZLC4ZXF5VO/9NmeUZXppv3r4/yb9k83N/f5L3VNXxJB/J4vLHb+6CuTPGeH5+Ppnkz7P44dj9PDmR5MQY47G5/7Eswt197h13JPnCGOPrc38jc68y1J9PcuN8tXxfFk8PHl7h+qvycJKdV1rvyeIa8M7xn5iv1r4zyan5lObRJLdV1RXzFd3b5rG1qKpK8rtJnhlj/Poumvuqqrp8bn93FtfVn8ki2O89y9w73897k3x6PiJ5OMmh+e6K65PcmORz65p7jHHfGOPaMcZWFufsp8cYP9597qq6rKreuLOdxb/fI2l+nowxXkjyXFXdNA/9UJIvd597yd15+bLHznzrn3vFF9nvzOJdCl9J8oFNXNg/xzwPJflakv/M4if5+7K4nvipJM8m+ZskV877VpIPztmfSnJwaZ2fSnJsfvzkmmd+VxZPn76U5In5cecumPt7k3xxzn0kyS/P4zdkEaxjWTxdvGQef/3cPzZvv2FprQ/M7+dokjs2eL78QF5+10frued8T86Pp3f+vnU/T+bXe1uS7Xmu/EUW737YDXNflsWzp/1LxzYyt18hB2juon4xEWA3EGqA5oQaoDmhBmhOqAGaE2qA5oQaoLn/Bkl4LaFXjXYhAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "print(data_df['label'].value_counts())\n",
    "data_df['label'].value_counts().plot(kind='barh')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "id": "SgiD5uAdJXEm"
   },
   "outputs": [],
   "source": [
    "def loaddicchar():\n",
    "    dic = {}\n",
    "    char1252 = 'à|á|ả|ã|ạ|ầ|ấ|ẩ|ẫ|ậ|ằ|ắ|ẳ|ẵ|ặ|è|é|ẻ|ẽ|ẹ|ề|ế|ể|ễ|ệ|đ|ì|í|ỉ|ĩ|ị|ò|ó|ỏ|õ|ọ|ồ|ố|ổ|ỗ|ộ|ờ|ớ|ở|ỡ|ợ|ù|ú|ủ|ũ|ụ|ừ|ứ|ử|ữ|ự|ỳ|ý|ỷ|ỹ|ỵ|À|Á|Ả|Ã|Ạ|Ầ|Ấ|Ẩ|Ẫ|Ậ|Ằ|Ắ|Ẳ|Ẵ|Ặ|È|É|Ẻ|Ẽ|Ẹ|Ề|Ế|Ể|Ễ|Ệ|Ì|Í|Ỉ|Ĩ|Ị|Ò|Ó|Ỏ|Õ|Ọ|Ồ|Ố|Ổ|Ỗ|Ộ|Ờ|Ớ|Ở|Ỡ|Ợ|Ù|Ú|Ủ|Ũ|Ụ|Ừ|Ứ|Ử|Ữ|Ự|Ỳ|Ý|Ỷ|Ỹ|Ỵ'.split(\n",
    "        '|')\n",
    "    charutf8 = \"à|á|ả|ã|ạ|ầ|ấ|ẩ|ẫ|ậ|ằ|ắ|ẳ|ẵ|ặ|è|é|ẻ|ẽ|ẹ|ề|ế|ể|ễ|ệ|đ|ì|í|ỉ|ĩ|ị|ò|ó|ỏ|õ|ọ|ồ|ố|ổ|ỗ|ộ|ờ|ớ|ở|ỡ|ợ|ù|ú|ủ|ũ|ụ|ừ|ứ|ử|ữ|ự|ỳ|ý|ỷ|ỹ|ỵ|À|Á|Ả|Ã|Ạ|Ầ|Ấ|Ẩ|Ẫ|Ậ|Ằ|Ắ|Ẳ|Ẵ|Ặ|È|É|Ẻ|Ẽ|Ẹ|Ề|Ế|Ể|Ễ|Ệ|Ì|Í|Ỉ|Ĩ|Ị|Ò|Ó|Ỏ|Õ|Ọ|Ồ|Ố|Ổ|Ỗ|Ộ|Ờ|Ớ|Ở|Ỡ|Ợ|Ù|Ú|Ủ|Ũ|Ụ|Ừ|Ứ|Ử|Ữ|Ự|Ỳ|Ý|Ỷ|Ỹ|Ỵ\".split(\n",
    "        '|')\n",
    "    for i in range(len(char1252)):\n",
    "        dic[char1252[i]] = charutf8[i]\n",
    "    return dic\n",
    " \n",
    " \n",
    "dicchar = loaddicchar()\n",
    " \n",
    "# Đưa toàn bộ dữ liệu qua hàm này để chuẩn hóa lại\n",
    "def convert_unicode(txt):\n",
    "    return re.sub(\n",
    "        r'à|á|ả|ã|ạ|ầ|ấ|ẩ|ẫ|ậ|ằ|ắ|ẳ|ẵ|ặ|è|é|ẻ|ẽ|ẹ|ề|ế|ể|ễ|ệ|đ|ì|í|ỉ|ĩ|ị|ò|ó|ỏ|õ|ọ|ồ|ố|ổ|ỗ|ộ|ờ|ớ|ở|ỡ|ợ|ù|ú|ủ|ũ|ụ|ừ|ứ|ử|ữ|ự|ỳ|ý|ỷ|ỹ|ỵ|À|Á|Ả|Ã|Ạ|Ầ|Ấ|Ẩ|Ẫ|Ậ|Ằ|Ắ|Ẳ|Ẵ|Ặ|È|É|Ẻ|Ẽ|Ẹ|Ề|Ế|Ể|Ễ|Ệ|Ì|Í|Ỉ|Ĩ|Ị|Ò|Ó|Ỏ|Õ|Ọ|Ồ|Ố|Ổ|Ỗ|Ộ|Ờ|Ớ|Ở|Ỡ|Ợ|Ù|Ú|Ủ|Ũ|Ụ|Ừ|Ứ|Ử|Ữ|Ự|Ỳ|Ý|Ỷ|Ỹ|Ỵ',\n",
    "        lambda x: dicchar[x.group()], txt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "id": "28hSajIdAgb1"
   },
   "outputs": [],
   "source": [
    "from vncorenlp import VnCoreNLP\n",
    "rdrsegmenter = VnCoreNLP(\"./vncorenlp/VnCoreNLP-1.1.1.jar\", annotators=\"wseg\", max_heap_size='-Xmx500m') \n",
    "\n",
    "uniChars = \"àáảãạâầấẩẫậăằắẳẵặèéẻẽẹêềếểễệđìíỉĩịòóỏõọôồốổỗộơờớởỡợùúủũụưừứửữựỳýỷỹỵÀÁẢÃẠÂẦẤẨẪẬĂẰẮẲẴẶÈÉẺẼẸÊỀẾỂỄỆĐÌÍỈĨỊÒÓỎÕỌÔỒỐỔỖỘƠỜỚỞỠỢÙÚỦŨỤƯỪỨỬỮỰỲÝỶỸỴÂĂĐÔƠƯ\"\n",
    "punc_re = '[^\\w'+uniChars+']'\n",
    "def preprocess_text(sen):\n",
    "  ##Unicode reform\n",
    "  assert isinstance(sen, str), 'Not String'\n",
    "  sen = convert_unicode(sen)\n",
    "  ##Remove punctuation\n",
    "  sen = re.sub(punc_re,' ', sen)\n",
    "  ##Remove multiple space\n",
    "  sen = re.sub('\\s+',' ', sen)\n",
    "  ##Lower\n",
    "  sen = sen.lower()\n",
    "  ##Segment\n",
    "  seg = rdrsegmenter.tokenize(sen)\n",
    "  if len(seg) == 0:\n",
    "    sen = ' '\n",
    "  else: \n",
    "    sen = ' '.join(seg[0])\n",
    "  return sen\n",
    "# vec_pre = np.vectorize(preprocess_text)\n",
    "# print(vec_pre((data_df.iloc[3,3])))\n",
    "data_df['preprocess_comment'] = data_df.apply(lambda x: preprocess_text(x['Comment']), axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 337
    },
    "id": "7AGP68b2jlTj",
    "outputId": "96c14495-ae54-4d80-c1ac-a09c9d731547"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>RevId</th>\n",
       "      <th>UserId</th>\n",
       "      <th>ResId</th>\n",
       "      <th>Comment</th>\n",
       "      <th>image_urls</th>\n",
       "      <th>score</th>\n",
       "      <th>label</th>\n",
       "      <th>preprocess_comment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>3648046</td>\n",
       "      <td>8920424</td>\n",
       "      <td>965165</td>\n",
       "      <td>Đặt dồi sụn của quán vì đọc comment thấy hấp d...</td>\n",
       "      <td>local1_folder-1/foody-doi-sun-pate-shop-online...</td>\n",
       "      <td>5.8</td>\n",
       "      <td>0</td>\n",
       "      <td>đặt dồi sụn của quán vì đọc comment thấy hấp_d...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>3695359</td>\n",
       "      <td>18558601</td>\n",
       "      <td>965165</td>\n",
       "      <td>Dồi khá ngon, mua về còn nóng mở ra thơm phức,...</td>\n",
       "      <td>local1_folder-1/foody-doi-sun-pate-shop-online...</td>\n",
       "      <td>9.0</td>\n",
       "      <td>1</td>\n",
       "      <td>dồi khá ngon mua về còn nóng mở ra thơm_phức s...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3695487</td>\n",
       "      <td>13885433</td>\n",
       "      <td>965165</td>\n",
       "      <td>Đặt xuất mỳ trộn thập cẩm, khá đầy đủ và đầy đ...</td>\n",
       "      <td>local1_folder-1/foody-doi-sun-pate-shop-online...</td>\n",
       "      <td>9.4</td>\n",
       "      <td>1</td>\n",
       "      <td>đặt xuất mỳ trộn thập_cẩm khá đầy_đủ và đầy_đặ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4256913</td>\n",
       "      <td>11400976</td>\n",
       "      <td>965165</td>\n",
       "      <td>Không hiểu sao quán này được 7.9 luôn. Đặt bán...</td>\n",
       "      <td>local1_folder-1/foody-doi-sun-pate-shop-online...</td>\n",
       "      <td>4.6</td>\n",
       "      <td>0</td>\n",
       "      <td>không hiểu sao quán này được 7 9 luôn đặt bánh...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4246644</td>\n",
       "      <td>525112</td>\n",
       "      <td>965165</td>\n",
       "      <td>Dồi sụn bé tẹo, giá quá cao so với các quán kh...</td>\n",
       "      <td>local1_folder-1/foody-doi-sun-pate-shop-online...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0</td>\n",
       "      <td>dồi sụn bé tẹo giá quá cao so với các quán khá...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     RevId    UserId   ResId  \\\n",
       "0  3648046   8920424  965165   \n",
       "1  3695359  18558601  965165   \n",
       "2  3695487  13885433  965165   \n",
       "3  4256913  11400976  965165   \n",
       "4  4246644    525112  965165   \n",
       "\n",
       "                                             Comment  \\\n",
       "0  Đặt dồi sụn của quán vì đọc comment thấy hấp d...   \n",
       "1  Dồi khá ngon, mua về còn nóng mở ra thơm phức,...   \n",
       "2  Đặt xuất mỳ trộn thập cẩm, khá đầy đủ và đầy đ...   \n",
       "3  Không hiểu sao quán này được 7.9 luôn. Đặt bán...   \n",
       "4  Dồi sụn bé tẹo, giá quá cao so với các quán kh...   \n",
       "\n",
       "                                          image_urls  score  label  \\\n",
       "0  local1_folder-1/foody-doi-sun-pate-shop-online...    5.8      0   \n",
       "1  local1_folder-1/foody-doi-sun-pate-shop-online...    9.0      1   \n",
       "2  local1_folder-1/foody-doi-sun-pate-shop-online...    9.4      1   \n",
       "3  local1_folder-1/foody-doi-sun-pate-shop-online...    4.6      0   \n",
       "4  local1_folder-1/foody-doi-sun-pate-shop-online...    1.0      0   \n",
       "\n",
       "                                  preprocess_comment  \n",
       "0  đặt dồi sụn của quán vì đọc comment thấy hấp_d...  \n",
       "1  dồi khá ngon mua về còn nóng mở ra thơm_phức s...  \n",
       "2  đặt xuất mỳ trộn thập_cẩm khá đầy_đủ và đầy_đặ...  \n",
       "3  không hiểu sao quán này được 7 9 luôn đặt bánh...  \n",
       "4  dồi sụn bé tẹo giá quá cao so với các quán khá...  "
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "0MjDRAeZbrpe",
    "outputId": "62b6abe7-41b0-450a-eb83-5d25e03f8128"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(9, 8)"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_df[data_df['preprocess_comment']==\" \"].shape\n",
    "# data_df.drop(data_df[data_df['preprocess_comment']==\" \"].index, inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "5t7qGKcjc8HS",
    "outputId": "3efb1572-d8bc-494d-cc79-911c341351ba"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4187    vị_trí ở mặt đường nhưng ở cạnh quán mì cay to...\n",
       "4188    thật tuyệt_cú mèo luôn mỗi cốc hoa_quả dầm ở đ...\n",
       "4189    mình đã ăn_ở địa_điểm 211 chùa láng được hơn 1...\n",
       "4190    nghe mọi người khen quán này dữ lém nên hôm_na...\n",
       "Name: preprocess_comment, dtype: object"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_df = data_df.reset_index()\n",
    "data_df['preprocess_comment'][4187:4191]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "_9pXfk9tBk_1",
    "outputId": "15337a24-3102-4db5-e239-38466aa2a289"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "314     1\n",
       "415     1\n",
       "744     1\n",
       "1011    1\n",
       "1077    1\n",
       "1543    1\n",
       "1779    1\n",
       "1808    1\n",
       "1888    1\n",
       "2302    1\n",
       "2337    1\n",
       "2528    1\n",
       "2836    1\n",
       "2873    1\n",
       "2976    1\n",
       "3489    1\n",
       "3643    1\n",
       "4328    1\n",
       "4409    1\n",
       "4645    1\n",
       "5242    1\n",
       "5564    1\n",
       "5976    1\n",
       "6341    1\n",
       "6487    1\n",
       "6748    1\n",
       "7554    1\n",
       "7603    1\n",
       "8684    1\n",
       "8881    1\n",
       "9377    1\n",
       "9523    1\n",
       "Name: preprocess_comment, dtype: int64"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = data_df['preprocess_comment'].str.split()\n",
    "a = a.apply(lambda x: len(x))\n",
    "a[a==1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ghhFsnQZRIOx",
    "outputId": "738871be-e8bd-4daa-a25f-a6441d005ee1"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0        56\n",
       "1        43\n",
       "2        38\n",
       "3        34\n",
       "4        74\n",
       "       ... \n",
       "9970     42\n",
       "9971     48\n",
       "9972    107\n",
       "9973    185\n",
       "9974     17\n",
       "Name: preprocess_comment, Length: 9975, dtype: int64"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "id": "uXAGoz9iQ2-0",
    "outputId": "b2d25fd1-ce31-4f08-eb56-6be297d2f662"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Sữa chua mít + trà đào ngon, caramen trong sữa chua ăn cũng ok, giá vừa phải. Quán sạch sẽ, nhân viên phục vụ tốt. Sẽ quay lại.'"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_df['Comment'][195]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 283
    },
    "id": "v0cC4OTiRXqG",
    "outputId": "37953fe6-4a70-4659-d9b6-19775faa850e"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<AxesSubplot:>"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "seq_len = [len(i.split()) for i in data_df['preprocess_comment']]\n",
    "\n",
    "pd.Series(seq_len).hist(bins = 30)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "W9S-CZXOtobN"
   },
   "source": [
    "# Load BPE, Vocab of PhoBERT and DataGen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 131,
     "referenced_widgets": [
      "2f8400d957d4428b86f73a07c12a69ff",
      "b375fdd33a8f4fad9f7dcc33f24f3f7b",
      "562df45b9b7e42799e8323f7db1eaf18",
      "624a86d43249405388f1286063cebae4",
      "c9918922671546a88e3b2ee509394df3",
      "1bb3780855494b029edc7dc564fcc51e",
      "046f564c5ccc4b9f9c866a6a57b900bf",
      "49da75829cbd4350a64bfcdc04576ac1",
      "4bea41d4adb249d5afc2cbd40dc3210d",
      "08e56fb9c8c74262aa75994d2766136d",
      "ca0da54234e9446db60b5331fa076d81",
      "1250687b6c4e48e0a4b79de74f294235",
      "0cfbf06467a04d4e9f97d8fbbb9328b7",
      "16736771e702494cb52be951b3df47bc",
      "df155493042740d9b96ee0343b9f03d4",
      "70da72f327b64a8a8d2574848ddaa5c0",
      "345e16e44bad4ccf83b61099383e8b71",
      "69e00ff1b849487f9e10000b486a766c",
      "98128e9bcb0b4cf4a949d51f299b3bde",
      "302b8bb010ec40908960c26742dd4df3",
      "3f3889c16b134b408de36adb46505c40",
      "db463b1c3ae44d6980bec291e144523c",
      "2889253c1e7d4df1b02db1773a304e26",
      "876c019d15f04031af2f5735ed37a038",
      "800ec21aa4674fce95f33685c2033b11",
      "6b4ab9aa406a4071b5e888b0cd25335c",
      "82995c53c15f4166a2641b2560faa49d",
      "1e21006c0b9740ecb6f427f6a49bf409",
      "733c53bc95c0490982e6473ad1d55418",
      "3d8af058291c4c7393bb04f2f1ec2622",
      "e4cb55de261c4416b945fdb72232c12d",
      "ad4a2e90f75d49f289b127d43acca1df",
      "d63ef1d399ab42698b667b2938d7e398"
     ]
    },
    "id": "6fqLCkKLZKoO",
    "outputId": "3f9ba7ba-a6e1-48e5-b3e3-cebab77d7c3c"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModel, AutoTokenizer\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"vinai/phobert-base\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "id": "UEUtZY6lbnhU"
   },
   "outputs": [],
   "source": [
    "def collate_batch(batch):\n",
    "  input_ids = []\n",
    "  attention_masks = []\n",
    "  imgs = []\n",
    "  img_len = []\n",
    "  labels = []\n",
    "  for bt in batch:\n",
    "    input_ids.append(bt[0]['input_ids'][0])\n",
    "    attention_masks.append(bt[0]['attention_mask'][0])\n",
    "    imgs.extend(bt[1])\n",
    "    img_len.append(len(bt[1]))\n",
    "    labels.append(bt[2])\n",
    "\n",
    "  bert_tokens = torch.stack(input_ids)\n",
    "  attention_masks = torch.stack(attention_masks)\n",
    "  imgs_torch = torch.stack(imgs)\n",
    "  labels = torch.LongTensor(labels)\n",
    "\n",
    "  return  { 'input_ids': bert_tokens , 'attention_mask':attention_masks } , imgs_torch, img_len, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "id": "NKgPUlOLlUx9"
   },
   "outputs": [],
   "source": [
    "trans = T.ToTensor()\n",
    "RANDOM_SEED = 0\n",
    "MAX_LEN = 150\n",
    "\n",
    "\n",
    "class SentimentData(Dataset):\n",
    "  def __init__(self, data, transform = None):\n",
    "    self.df = data.reset_index(drop=True)\n",
    "    self.transform = transform\n",
    "  def __len__(self):\n",
    "    # print('Size',len(self.df))\n",
    "    return len(self.df)  \n",
    "  def __getitem__(self,idx):\n",
    "    text = self.df[\"preprocess_comment\"][idx]\n",
    "    text = tokenizer(text, padding='max_length', truncation=True, max_length = MAX_LEN, return_tensors=\"pt\")\n",
    "\n",
    "    img_list = []\n",
    "    for path in self.df['image_urls'][idx].split(\",\"):\n",
    "#       img_path = '/' + path.split('/')[-1]\n",
    "        if path in data_zip.namelist():\n",
    "          img = Image.open(data_zip.open(path))\n",
    "          if self.transform is not None:\n",
    "            img = self.transform(img)\n",
    "          img_list.append(img)\n",
    "#         else:\n",
    "#           img = Image.open(bin_zip.open(path))\n",
    "#           if self.transform is not None:\n",
    "#             img = self.transform(img)\n",
    "#           img_list.append(img)\n",
    "\n",
    "    # print(text.shape)\n",
    "    # text = ' '.join(text[0])\n",
    "    label = self.df[\"label\"][idx]\n",
    "\n",
    "    return (text, img_list, label)\n",
    "\n",
    "class SentimentDataModule(pl.LightningDataModule):\n",
    "    \n",
    "  def __init__(self, data, batch_size: int = 8, test_size = 0.2):\n",
    "      super().__init__()\n",
    "      self.test_size = test_size\n",
    "      self.data = data\n",
    "      self.batch_size = batch_size\n",
    "      self.train_data = ''\n",
    "      self.val_data = ''\n",
    "      self.test_data = ''\n",
    "    \n",
    "#   def insert_bin_image(self, df):\n",
    "#     sample_df = df.sample(frac=0.4, random_state=0)\n",
    "#     df.drop(sample_df.index, inplace=True)\n",
    "#     df = df.reset_index(drop=True)\n",
    "#     sample_df = sample_df.reset_index(drop=True)\n",
    "#     for i in range(len(sample_df)):\n",
    "#         list_img = []\n",
    "#         for j in range(len(sample_df.iloc[i]['image_urls'].split(','))):\n",
    "#             num_img = ''\n",
    "#             img = ''\n",
    "#             while True:\n",
    "#                 num_ran = random.randint(1, 900)\n",
    "#                 num_img = '0'* (6-len(str(num_ran))) + str(num_ran)\n",
    "#                 img = Image.open(bin_zip.open('mountains/mountains_'+ num_img + '.png'))\n",
    "# #                 print(len(np.asarray(img).shape))\n",
    "#                 if len(np.asarray(img).shape) == 3:\n",
    "#                     break;\n",
    "#             list_img.append('mountains/mountains_'+ num_img + '.png')\n",
    "#         sample_df.loc[i,'image_urls'] = ','.join(list_img)\n",
    "    \n",
    "#     df = pd.concat([df,sample_df], ignore_index = True)\n",
    "    \n",
    "#     return df\n",
    "\n",
    "  def prepare_data(self):\n",
    "      self.train_data, self.val_data = train_test_split(self.data, test_size = self.test_size, random_state=RANDOM_SEED, stratify = self.data['label'])\n",
    "#       self.train_data = self.insert_bin_image(self.train_data)\n",
    "#       print(self.train_data)\n",
    "      self.test_data, self.val_data = train_test_split(self.val_data,test_size = 0.5, random_state=RANDOM_SEED, stratify = self.val_data['label'])\n",
    "      print(self.data['label'].value_counts())\n",
    "      print(self.train_data['label'].value_counts())\n",
    "      print(self.val_data['label'].value_counts())\n",
    "      print(self.test_data['label'].value_counts())\n",
    "\n",
    "  def setup(self, stage = None):\n",
    "      train_transform = T.Compose([\n",
    "       T.Resize([299, 299]), T.RandomHorizontalFlip(), T.RandomRotation(10),\n",
    "        T.ToTensor(),\n",
    "        T.Normalize(mean=[0.485, 0.456, 0.406],\n",
    "                    std=[0.229, 0.224, 0.225])\n",
    "      ])\n",
    "      val_transform = T.Compose([\n",
    "        T.Resize([299, 299]), T.ToTensor(),\n",
    "        T.Normalize(mean=[0.485, 0.456, 0.406],\n",
    "                    std=[0.229, 0.224, 0.225])\n",
    "      ])\n",
    "      test_transform = T.Compose([\n",
    "        T.Resize([299, 299]), T.ToTensor(),\n",
    "        T.Normalize(mean=[0.485, 0.456, 0.406],\n",
    "                    std=[0.229, 0.224, 0.225])\n",
    "      ])\n",
    "      self.train = SentimentData(self.train_data, train_transform)\n",
    "      self.val = SentimentData(self.val_data, val_transform)\n",
    "      self.test = SentimentData(self.test_data, test_transform)\n",
    "\n",
    "\n",
    "  def train_dataloader(self):\n",
    "      return DataLoader(self.train, batch_size=self.batch_size, shuffle=True, collate_fn=collate_batch)\n",
    "\n",
    "  def val_dataloader(self):\n",
    "      return DataLoader(self.val, batch_size=self.batch_size, collate_fn=collate_batch)\n",
    "\n",
    "  def test_dataloader(self):\n",
    "      return DataLoader(self.test, batch_size=self.batch_size, collate_fn=collate_batch)\n",
    "\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "DbSBBPTGf8d4",
    "outputId": "797072df-fabd-4599-a19d-7c3ad9236ba4"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1    6952\n",
      "0    3023\n",
      "Name: label, dtype: int64\n",
      "1    5562\n",
      "0    2418\n",
      "Name: label, dtype: int64\n",
      "1    695\n",
      "0    303\n",
      "Name: label, dtype: int64\n",
      "1    695\n",
      "0    302\n",
      "Name: label, dtype: int64\n",
      "0\n",
      "torch.Size([8, 150])\n",
      "torch.Size([11, 3, 299, 299])\n",
      "11\n",
      "8\n"
     ]
    }
   ],
   "source": [
    "# Do some Test with data\n",
    "if __name__ == \"__main__\":\n",
    "  dm = SentimentDataModule(data_df)\n",
    "  dm.prepare_data()\n",
    "  dm.setup()\n",
    "  for step, bat in enumerate(dm.train_dataloader()):\n",
    "    if step > 0: break\n",
    "    print(step)\n",
    "    print(bat[0]['input_ids'].shape)\n",
    "    print(bat[1].shape)\n",
    "    print(sum(bat[2]))\n",
    "    print(len(bat[3]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>index</th>\n",
       "      <th>RevId</th>\n",
       "      <th>UserId</th>\n",
       "      <th>ResId</th>\n",
       "      <th>Comment</th>\n",
       "      <th>image_urls</th>\n",
       "      <th>score</th>\n",
       "      <th>label</th>\n",
       "      <th>preprocess_comment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>521</th>\n",
       "      <td>521</td>\n",
       "      <td>2659214</td>\n",
       "      <td>13591898</td>\n",
       "      <td>920318</td>\n",
       "      <td>Có đủ các loại trà mà mình thích nhất matcha l...</td>\n",
       "      <td>local1_folder-1/foody-tra-sua-mancha-tea-doan-...</td>\n",
       "      <td>8.6</td>\n",
       "      <td>1</td>\n",
       "      <td>có đủ các loại trà mà mình thích nhất matcha l...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8362</th>\n",
       "      <td>8362</td>\n",
       "      <td>2585742</td>\n",
       "      <td>227729</td>\n",
       "      <td>876305</td>\n",
       "      <td>có code giảm giá nên đặt thử, nói chung cơm ăn...</td>\n",
       "      <td>local1_folder-1/foody-minimo-com-mi-692-636905...</td>\n",
       "      <td>9.0</td>\n",
       "      <td>1</td>\n",
       "      <td>có code giảm_giá nên đặt thử nói_chung cơm ăn ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4635</th>\n",
       "      <td>4635</td>\n",
       "      <td>9182412</td>\n",
       "      <td>2261673</td>\n",
       "      <td>1052111</td>\n",
       "      <td>Thấy quán mới mở nên hôm nay gọi mì để thẩm xe...</td>\n",
       "      <td>local1_folder-1/foody-mi-tron-532-truong-chinh...</td>\n",
       "      <td>9.2</td>\n",
       "      <td>1</td>\n",
       "      <td>thấy quán mới mở nên hôm_nay gọi mì để thẩm xe...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>519</th>\n",
       "      <td>519</td>\n",
       "      <td>2843157</td>\n",
       "      <td>14644373</td>\n",
       "      <td>295582</td>\n",
       "      <td>Thật sự thì với cái giá 20k mà được một bát ch...</td>\n",
       "      <td>local1_folder-1/foody-chao-long-ba-hong-853-63...</td>\n",
       "      <td>4.8</td>\n",
       "      <td>0</td>\n",
       "      <td>thật_sự thì với cái giá 20k mà được một bát ch...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8150</th>\n",
       "      <td>8150</td>\n",
       "      <td>1111812</td>\n",
       "      <td>1477995</td>\n",
       "      <td>122539</td>\n",
       "      <td>Mình cũng chẳng biết đây có phải địa chỉ đúng ...</td>\n",
       "      <td>local1_folder-1/foody-tra-chanh-nha-tho-286-63...</td>\n",
       "      <td>5.6</td>\n",
       "      <td>0</td>\n",
       "      <td>mình cũng chẳng biết đây có phải địa_chỉ đúng ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2417</th>\n",
       "      <td>2417</td>\n",
       "      <td>2835833</td>\n",
       "      <td>10264997</td>\n",
       "      <td>881083</td>\n",
       "      <td>Mì ăn ngon, hơi ít thịt nhưng giá 30k nên cũng...</td>\n",
       "      <td>local1_folder-1/foody-dung-beo-banh-mi-doner-k...</td>\n",
       "      <td>5.6</td>\n",
       "      <td>0</td>\n",
       "      <td>mì ăn ngon hơi ít thịt nhưng giá 30k nên cũng ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5115</th>\n",
       "      <td>5115</td>\n",
       "      <td>7505366</td>\n",
       "      <td>589693</td>\n",
       "      <td>704512</td>\n",
       "      <td>Đọc kha khá review khem quán nên mặc dù hơi xa...</td>\n",
       "      <td>local1_folder-1/foody-smile-tea-coffee-908-637...</td>\n",
       "      <td>4.2</td>\n",
       "      <td>0</td>\n",
       "      <td>đọc kha_khá review khem quán nên mặc_dù hơi xa...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7718</th>\n",
       "      <td>7718</td>\n",
       "      <td>2636993</td>\n",
       "      <td>12045615</td>\n",
       "      <td>712278</td>\n",
       "      <td>Tớ khó tính khi ăn thịt xá xíu cực kì, chỉ ưng...</td>\n",
       "      <td>local1_folder-1/foody-duc-lam-com-rang-com-dao...</td>\n",
       "      <td>9.0</td>\n",
       "      <td>1</td>\n",
       "      <td>tớ khó_tính khi ăn thịt xá_xíu cực_kì chỉ ưng ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4479</th>\n",
       "      <td>4479</td>\n",
       "      <td>2359199</td>\n",
       "      <td>12383305</td>\n",
       "      <td>708526</td>\n",
       "      <td>Lẩu xuất sắc\\nDễ tìm\\nPhục vụ nhanh nhẹn\\nBia ...</td>\n",
       "      <td>local1_folder-1/foody-quan-289-lau-nuong-com-v...</td>\n",
       "      <td>9.2</td>\n",
       "      <td>1</td>\n",
       "      <td>lẩu xuất_sắc dễ tìm phục_vụ nhanh_nhẹn bia ngo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5152</th>\n",
       "      <td>5152</td>\n",
       "      <td>2635053</td>\n",
       "      <td>931351</td>\n",
       "      <td>895351</td>\n",
       "      <td>Ấn tượng về quán là decor đẹp dã man và quán c...</td>\n",
       "      <td>local1_folder-1/foody-quan-cua-minh-coffee-114...</td>\n",
       "      <td>9.0</td>\n",
       "      <td>1</td>\n",
       "      <td>ấn_tượng về quán là decor đẹp dã_man và quán c...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>401 rows × 9 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      index    RevId    UserId    ResId  \\\n",
       "521     521  2659214  13591898   920318   \n",
       "8362   8362  2585742    227729   876305   \n",
       "4635   4635  9182412   2261673  1052111   \n",
       "519     519  2843157  14644373   295582   \n",
       "8150   8150  1111812   1477995   122539   \n",
       "...     ...      ...       ...      ...   \n",
       "2417   2417  2835833  10264997   881083   \n",
       "5115   5115  7505366    589693   704512   \n",
       "7718   7718  2636993  12045615   712278   \n",
       "4479   4479  2359199  12383305   708526   \n",
       "5152   5152  2635053    931351   895351   \n",
       "\n",
       "                                                Comment  \\\n",
       "521   Có đủ các loại trà mà mình thích nhất matcha l...   \n",
       "8362  có code giảm giá nên đặt thử, nói chung cơm ăn...   \n",
       "4635  Thấy quán mới mở nên hôm nay gọi mì để thẩm xe...   \n",
       "519   Thật sự thì với cái giá 20k mà được một bát ch...   \n",
       "8150  Mình cũng chẳng biết đây có phải địa chỉ đúng ...   \n",
       "...                                                 ...   \n",
       "2417  Mì ăn ngon, hơi ít thịt nhưng giá 30k nên cũng...   \n",
       "5115  Đọc kha khá review khem quán nên mặc dù hơi xa...   \n",
       "7718  Tớ khó tính khi ăn thịt xá xíu cực kì, chỉ ưng...   \n",
       "4479  Lẩu xuất sắc\\nDễ tìm\\nPhục vụ nhanh nhẹn\\nBia ...   \n",
       "5152  Ấn tượng về quán là decor đẹp dã man và quán c...   \n",
       "\n",
       "                                             image_urls  score  label  \\\n",
       "521   local1_folder-1/foody-tra-sua-mancha-tea-doan-...    8.6      1   \n",
       "8362  local1_folder-1/foody-minimo-com-mi-692-636905...    9.0      1   \n",
       "4635  local1_folder-1/foody-mi-tron-532-truong-chinh...    9.2      1   \n",
       "519   local1_folder-1/foody-chao-long-ba-hong-853-63...    4.8      0   \n",
       "8150  local1_folder-1/foody-tra-chanh-nha-tho-286-63...    5.6      0   \n",
       "...                                                 ...    ...    ...   \n",
       "2417  local1_folder-1/foody-dung-beo-banh-mi-doner-k...    5.6      0   \n",
       "5115  local1_folder-1/foody-smile-tea-coffee-908-637...    4.2      0   \n",
       "7718  local1_folder-1/foody-duc-lam-com-rang-com-dao...    9.0      1   \n",
       "4479  local1_folder-1/foody-quan-289-lau-nuong-com-v...    9.2      1   \n",
       "5152  local1_folder-1/foody-quan-cua-minh-coffee-114...    9.0      1   \n",
       "\n",
       "                                     preprocess_comment  \n",
       "521   có đủ các loại trà mà mình thích nhất matcha l...  \n",
       "8362  có code giảm_giá nên đặt thử nói_chung cơm ăn ...  \n",
       "4635  thấy quán mới mở nên hôm_nay gọi mì để thẩm xe...  \n",
       "519   thật_sự thì với cái giá 20k mà được một bát ch...  \n",
       "8150  mình cũng chẳng biết đây có phải địa_chỉ đúng ...  \n",
       "...                                                 ...  \n",
       "2417  mì ăn ngon hơi ít thịt nhưng giá 30k nên cũng ...  \n",
       "5115  đọc kha_khá review khem quán nên mặc_dù hơi xa...  \n",
       "7718  tớ khó_tính khi ăn thịt xá_xíu cực_kì chỉ ưng ...  \n",
       "4479  lẩu xuất_sắc dễ tìm phục_vụ nhanh_nhẹn bia ngo...  \n",
       "5152  ấn_tượng về quán là decor đẹp dã_man và quán c...  \n",
       "\n",
       "[401 rows x 9 columns]"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dm.train_data[7579:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "qiJlzVNbZ7f7",
    "outputId": "5243a986-b1f1-449c-bb6c-2cdd1a314eb4"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Computation device: cuda\n",
      "\n"
     ]
    }
   ],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Computation device: {device}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FknwjXWwJfkL"
   },
   "source": [
    "# PhoBERT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "id": "u9BWfj8BGmN5"
   },
   "outputs": [],
   "source": [
    "from transformers import RobertaModel, RobertaConfig, AdamW, AutoModel\n",
    "from transformers import RobertaForSequenceClassification\n",
    "\n",
    "pretrained_config_path = \"/content/drive/MyDrive/NLP/PhoBERT_base_transformers/config.json\"\n",
    "pretrained_model_path = \"/content/drive/MyDrive/NLP/PhoBERT_base_transformers/model.bin\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "id": "S18HY-AOn8d3"
   },
   "outputs": [],
   "source": [
    "class Bert_Lstm(nn.Module):\n",
    "  def __init__(self):\n",
    "    super().__init__()\n",
    "    self.PhoBERT = AutoModel.from_pretrained(\"vinai/phobert-base\")\n",
    "\n",
    "    self.lstm = nn.LSTM(input_size=768, hidden_size=128, num_layers= 1, batch_first=True)\n",
    "  \n",
    "  def forward(self, texts):\n",
    "    bert_out = self.PhoBERT(texts['input_ids'],token_type_ids=None, attention_mask = texts['attention_mask'])\n",
    "    bert_out = bert_out['last_hidden_state']\n",
    "    out, _ = self.lstm(bert_out)\n",
    "    feature_extract = out[:,-1,:].contiguous()\n",
    "\n",
    "    return feature_extract"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "id": "f0oZbzl0rb1F"
   },
   "outputs": [],
   "source": [
    "# # This time, the output's 2nd dimension is 3, indicating that there were 3 outputs given by the LSTM. \n",
    "# # This corresponds to the length of our input sequence. \n",
    "# # For the use cases where we'll need an output at every time step (many-to-many), such as Text Generation, the output of each time step can be extracted directly from the 2nd dimension and fed into a fully connected layer. \n",
    "# # For text classification tasks (many-to-one), such as Sentiment Analysis, the last output can be taken to be fed into a classifier.\n",
    "\n",
    "# pho_test = Bert_Lstm()\n",
    "# pho_test\n",
    "# # oooo = pho_test('iiii')\n",
    "# for name, child in pho_test.named_children():\n",
    "#   print(name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lzr3M8PIIo6m"
   },
   "source": [
    "# InceptionV3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "id": "O-70eEW4KtHd"
   },
   "outputs": [],
   "source": [
    "from torchvision.models import inception_v3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "id": "qpksgL5AdyYG"
   },
   "outputs": [],
   "source": [
    "# class Inception(nn.Module):\n",
    "#   def __init__(self):\n",
    "#     super(Inception,self).__init__()\n",
    "#     self.incept = inception_v3(pretrained = True,progress = True,aux_logits= False,transform_input = True)\n",
    "#     self.incept_sequen = nn.Sequential(*list(self.incept.children())[:-3])\n",
    "#     self.avgpool = nn.AdaptiveAvgPool2d((1,1))\n",
    "#     self.drop = nn.Dropout(p=0.4)\n",
    "#     self.flat = nn.Linear(2048, 128)\n",
    "\n",
    "#   def process(self, feature, len_img_list):\n",
    "#     max_fea = torch.zeros(len(len_img_list), feature.shape[-1]).to(device)\n",
    "#     start = 0\n",
    "#     for idx, num in enumerate(len_img_list):\n",
    "#       max_fea[idx] = feature[start:start+num].max(0)[0]\n",
    "#       start += num\n",
    "\n",
    "#     return max_fea\n",
    "  \n",
    "\n",
    "#   def forward(self, image, len_img_list):\n",
    "#     feature = self.incept_sequen(image)\n",
    "#     feature = self.avgpool(feature)\n",
    "#     feature = self.drop(feature)\n",
    "#     feature = feature.reshape(-1, 2048)\n",
    "# #     feature = self.drop(feature)\n",
    "#     # out = self.avgpool(feature)\n",
    "#     # print(out.shape)\n",
    "#     # out = out.contiguous().view(-1, 2048)\n",
    "#     out = self.process(feature, len_img_list)\n",
    "#     img_fea = self.flat(out)\n",
    "\n",
    "#     return img_fea\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "# img_fea = torch.rand(4,128).to(device)\n",
    "# img_fea.shape\n",
    "# text_fea = torch.rand(2,128).to(device)\n",
    "# len_img_list=[2,2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def attention(image_fea, text_fea, len_img_list):\n",
    "#     m = nn.Softmax(dim=0)\n",
    "#     scale = 1.0/np.sqrt(128)\n",
    "# #     context_vector = torch.mul(text_fea, m(torch.matmul(text_fea, img_fea.permute(1,0))))\n",
    "#     context_vector = torch.zeros(len(len_img_list), 128).to(device)\n",
    "#     start_term = 0\n",
    "#     for i in range(len(len_img_list)):\n",
    "#         context_vector[i] = torch.mul(text_fea[i], m(torch.matmul(image_fea[start_term: start_term + len_img_list[i]], text_fea[i].contiguous().view(128,-1))*scale)).sum(dim=0)\n",
    "#         start_term += len_img_list[i]\n",
    "#     return context_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "# m = nn.Softmax(dim=0)\n",
    "# scale = 1.0/np.sqrt(128)\n",
    "# m(torch.matmul(img_fea[0: 2], text_fea[0].contiguous().view(128,-1))*scale)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(img_fea[0: 2].shape)\n",
    "# img_fea[0: 2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "# torch.mul(img_fea[0:2], m(torch.matmul(img_fea[0: 2], text_fea[0].contiguous().view(128,-1))*scale))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "# torch.matmul(img_fea[0: 2], text_fea[0].contiguous().view(128,-1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "# context = attention(img_fea, text_fea, len_img_list)\n",
    "# context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "# context.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Inception(nn.Module):\n",
    "  def __init__(self):\n",
    "    super(Inception,self).__init__()\n",
    "    self.incept = inception_v3(pretrained = True,progress = True,aux_logits= False,transform_input = True)\n",
    "    self.incept_sequen = nn.Sequential(*list(self.incept.children())[:-3])\n",
    "    self.avgpool = nn.AdaptiveMaxPool2d((1,1))\n",
    "    self.drop = nn.Dropout(p=0.4)\n",
    "#     self.flat = nn.Linear(2048, 128)\n",
    "    self.ffn = nn.Sequential(\n",
    "            nn.Linear(2048, 512),\n",
    "            nn.ReLU(),\n",
    "#             nn.Dropout(0.3),\n",
    "            nn.Linear(512, 128),\n",
    "            nn.ReLU(),\n",
    "#             nn.Dropout(0.2),\n",
    "#             nn.Linear(128, 2),\n",
    "        )\n",
    "    self.atten = nn.Linear(128, 1, bias=False)\n",
    "#     self.cl = nn.Linear(128,2)\n",
    "  def attention(self, image_fea, atten_coef ,len_img_list):\n",
    "    m = nn.Softmax(dim=0)\n",
    "    scale = 1.0/np.sqrt(128)\n",
    "#     context_vector = torch.mul(text_fea, m(torch.matmul(text_fea, img_fea.permute(1,0))))\n",
    "    context_vector = torch.zeros(len(len_img_list), 128).to(device)\n",
    "    start_term = 0\n",
    "    for i in range(len(len_img_list)):\n",
    "        context_vector[i] = torch.mul(image_fea[start_term: start_term + len_img_list[i]], m(atten_coef[start_term: start_term + len_img_list[i]]*scale)).sum(dim=0)\n",
    "        start_term += len_img_list[i]\n",
    "    return context_vector\n",
    "  \n",
    "  def forward(self, image, len_img_list):\n",
    "    image = image.to(device)\n",
    "    \n",
    "    feature = self.incept_sequen(image)\n",
    "    feature = self.avgpool(feature)\n",
    "    feature = self.drop(feature)\n",
    "\n",
    "    img_fea = feature.reshape(-1, 2048)\n",
    "    out = self.ffn(img_fea)\n",
    "    atten_coef = self.atten(out)\n",
    "    \n",
    "    context = self.attention(out, atten_coef, len_img_list)\n",
    "#     logits = self.cl(context)\n",
    "    \n",
    "    return context\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Inception(nn.Module):\n",
    "  def __init__(self):\n",
    "    super(Inception,self).__init__()\n",
    "    self.incept = inception_v3(pretrained = True,progress = True,aux_logits= False,transform_input = True)\n",
    "    self.incept_sequen = nn.Sequential(*list(self.incept.children())[:-3])\n",
    "    self.avgpool = nn.AdaptiveAvgPool2d((1,1))\n",
    "    self.drop = nn.Dropout(p=0.4)\n",
    "#     self.flat = nn.Linear(2048, 128)\n",
    "    self.ffn = nn.Sequential(\n",
    "            nn.Linear(2048, 512),\n",
    "            nn.ReLU(),\n",
    "#             nn.Dropout(0.3),\n",
    "            nn.Linear(512, 128),\n",
    "            nn.ReLU(),\n",
    "#             nn.Dropout(0.2),\n",
    "#             nn.Linear(128, 2),\n",
    "        )\n",
    "  \n",
    "  def forward(self, image, len_img_list):\n",
    "    feature = self.incept_sequen(image)\n",
    "    feature = self.avgpool(feature)\n",
    "    feature = self.drop(feature)\n",
    "\n",
    "    img_fea = feature.reshape(-1, 2048)\n",
    "    out = self.ffn(img_fea)\n",
    "\n",
    "    \n",
    "    return out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "# inc = inception_v3(pretrained = True,progress = True,aux_logits= False,transform_input = False)\n",
    "# inc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "# incept_sequen = nn.Sequential(*list(inc.children())[:-3])\n",
    "# incept_sequen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "id": "owCLI2adVEoV"
   },
   "outputs": [],
   "source": [
    "# from tensorflow.keras.applications.inception_v3 import InceptionV3\n",
    "# from tensorflow.keras import layers\n",
    "# from tensorflow.keras import models\n",
    "# from tensorflow.keras.models import Model\n",
    "\n",
    "# basemodel = InceptionV3(weights='imagenet', include_top=False, input_tensor=layers.Input(shape=(299, 299, 3)))\n",
    "# x = basemodel.output\n",
    "# # t = layers.AveragePooling2D(pool_size=(8, 8), name='AVG_Pooling')(x)\n",
    "# model_cnn = Model(basemodel.input, x)\n",
    "\n",
    "# basemodel.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "id": "U0E4iJyHdDUN"
   },
   "outputs": [],
   "source": [
    "# ou = model_cnn(np.random.rand(1,299,299,3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "id": "r7_2g1eNhBWe"
   },
   "outputs": [],
   "source": [
    "# ou.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JEsEq9RHfbCV"
   },
   "source": [
    "# Multimodal (no pytorch-lightning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "id": "hmIwJsEOlt8h"
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_auc_score, classification_report, accuracy_score\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "# writer = SummaryWriter('runs/mrtrongmodel')\n",
    "\n",
    "class LRScheduler():\n",
    "    def __init__(\n",
    "        self, optimizer, patience=1, min_lr=1e-6, factor=0.1\n",
    "    ):\n",
    "        self.optimizer = optimizer\n",
    "        self.patience = patience\n",
    "        self.min_lr = min_lr\n",
    "        self.factor = factor\n",
    "        self.lr_scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau( \n",
    "                self.optimizer,\n",
    "                mode='max',\n",
    "                patience=self.patience,\n",
    "                factor=self.factor,\n",
    "                min_lr=self.min_lr,\n",
    "                verbose=True\n",
    "            )\n",
    "    def __call__(self, val_loss):\n",
    "        self.lr_scheduler.step(val_loss)\n",
    "\n",
    "class EarlyStopping():\n",
    "    def __init__(self, patience=8, min_delta=0):\n",
    "        self.patience = patience\n",
    "        self.min_delta = min_delta\n",
    "        self.counter = 0\n",
    "        self.best_loss = None\n",
    "        self.early_stop = False\n",
    "    def __call__(self, val_loss):\n",
    "        if self.best_loss == None:\n",
    "            self.best_loss = val_loss\n",
    "        elif self.best_loss - val_loss > self.min_delta:\n",
    "            self.best_loss = val_loss\n",
    "            # reset counter if validation loss improves\n",
    "            self.counter = 0\n",
    "        elif self.best_loss - val_loss < self.min_delta:\n",
    "            self.counter += 1\n",
    "            print(f\"INFO: Early stopping counter {self.counter} of {self.patience}\")\n",
    "            if self.counter >= self.patience:\n",
    "                print('INFO: Early stopping')\n",
    "                self.early_stop = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "id": "mGO97xYhcuNV"
   },
   "outputs": [],
   "source": [
    "class Concatmodal(nn.Module):\n",
    "  def __init__(self):\n",
    "    super(Concatmodal,self).__init__()\n",
    "    # self.loss_module = nn.CrossEntropyLoss()\n",
    "    self.BERT = Bert_Lstm()\n",
    "    self.incept = Inception()\n",
    "    self.dense = nn.Linear(256,256)\n",
    "    self.cl = nn.Linear(256,2)\n",
    "    # self.bpe_path = \"/content/drive/MyDrive/NLP/PhoBERT_base_transformers/bpe.codes\"\n",
    "    # self.vocab_path = \"/content/drive/MyDrive/NLP/PhoBERT_base_transformers/dict.txt\"\n",
    "    \n",
    "#   def attention(self, image_fea, text_fea, len_img_list):\n",
    "#     m = nn.Softmax(dim=0)\n",
    "#     scale = 1.0/np.sqrt(128)\n",
    "# #     context_vector = torch.mul(text_fea, m(torch.matmul(text_fea, img_fea.permute(1,0))))\n",
    "#     context_vector = torch.zeros(len(len_img_list), 128).to(device)\n",
    "#     start_term = 0\n",
    "#     for i in range(len(len_img_list)):\n",
    "#         context_vector[i] = torch.mul(image_fea[start_term: start_term + len_img_list[i]], m(torch.matmul(image_fea[start_term: start_term + len_img_list[i]], text_fea[i].contiguous().view(128,-1))*scale)).sum(dim=0)\n",
    "#         start_term += len_img_list[i]\n",
    "#     return context_vector\n",
    "        \n",
    "  def forward(self, image, inputs, len_img_list, labels=None):\n",
    "    for key in inputs:\n",
    "        inputs[key] = inputs[key].to(device)\n",
    "    image = image.to(device)\n",
    "\n",
    "    fea1 = self.BERT(inputs)\n",
    "    fea2 = self.incept(image, len_img_list)\n",
    "    \n",
    "#     context_vec = self.attention(fea2, fea1,  len_img_list)\n",
    "    cat = torch.cat((fea1, fea2), 1)\n",
    "    out = self.cl(cat)\n",
    "\n",
    "    return out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "id": "BWqjbaCdq6nz"
   },
   "outputs": [],
   "source": [
    "def fit(model, dm, criterion, optimizer, epoch):\n",
    "  running_loss = 0.0\n",
    "  train_running_loss = 0.0\n",
    "  # running_correct = 0\n",
    "  reporting_step = 100\n",
    "  train_preds = np.array([])\n",
    "  train_labels = np.array([])\n",
    "\n",
    "  counter = 0\n",
    "  # total = 0\n",
    "  soft_m = nn.Softmax(dim=-1)\n",
    "\n",
    "  model.train()\n",
    "\n",
    "  for i, (texts, images, len_img_list, labels) in enumerate(dm.train_dataloader()):\n",
    "      counter += 1\n",
    "      labels = labels.to(device)\n",
    "      optimizer.zero_grad()\n",
    "\n",
    "      outputs = model(images, texts, len_img_list)\n",
    "      loss = criterion(outputs, labels)\n",
    "      output_scores = soft_m(outputs)\n",
    "      predictions = torch.argmax(output_scores, dim=-1)\n",
    "\n",
    "      loss.backward()\n",
    "      optimizer.step()\n",
    "\n",
    "      running_loss += loss.item()\n",
    "      train_running_loss += loss.item()\n",
    "        \n",
    "      predictions = predictions.cpu().numpy()\n",
    "      labels = labels.cpu().numpy()\n",
    "      # _, preds = torch.max(outputs.data, 1)\n",
    "      train_preds = np.concatenate((train_preds, predictions), axis=0)\n",
    "      train_labels = np.concatenate((train_labels, labels), axis=0)\n",
    "\n",
    "      if i % reporting_step == reporting_step-1:\n",
    "            print(f\"Epoch {epoch} Step {i} ave_loss {running_loss/reporting_step:0.4f}\")\n",
    "            running_loss = 0.0\n",
    "            \n",
    "  train_loss = train_running_loss / counter\n",
    "  reports = classification_report(train_labels, train_preds, output_dict=True)\n",
    "  print(confusion_matrix(train_labels, train_preds))\n",
    "  train_ac = classification_report(train_labels, train_preds, output_dict=True)['macro avg']['f1-score']\n",
    "    \n",
    "#   writer.add_scalar(\"Loss/train\", train_loss, epoch)\n",
    "#   writer.add_scalar(\"macro-f1/train\", reports['macro avg']['f1-score'], epoch)\n",
    "#   writer.add_scalar(\"Acc/train\", train_ac, epoch)\n",
    "\n",
    "\n",
    "  return train_loss, train_ac"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "id": "Fu5J5PqcvamM"
   },
   "outputs": [],
   "source": [
    "def validation(model, dm, criterion):\n",
    "  model.eval()\n",
    "  val_running_loss = 0.0\n",
    "  val_preds = np.array([])\n",
    "  val_labels = np.array([])\n",
    "\n",
    "  soft_m = nn.Softmax(dim=-1)\n",
    "\n",
    "  counter = 0\n",
    "\n",
    "  with torch.no_grad():\n",
    "    for i, (texts, images, len_img_list, labels) in enumerate(dm.val_dataloader()):\n",
    "        counter += 1\n",
    "        labels = labels.to(device)\n",
    "\n",
    "        outputs = model(images, texts, len_img_list)\n",
    "\n",
    "        loss = criterion(outputs, labels)\n",
    "        output_scores = soft_m(outputs)\n",
    "        predictions = torch.argmax(output_scores, dim=-1)\n",
    "\n",
    "        predictions = predictions.cpu().numpy()\n",
    "        labels = labels.cpu().numpy()\n",
    "        \n",
    "        val_running_loss += loss.item()\n",
    "        val_preds = np.concatenate((val_preds, predictions), axis=0)\n",
    "        val_labels = np.concatenate((val_labels, labels), axis=0)\n",
    "    \n",
    "\n",
    "  val_loss = val_running_loss / counter\n",
    "  print(classification_report(val_labels, val_preds))\n",
    "  print(confusion_matrix(val_labels, val_preds))\n",
    "#   val_ac = accuracy_score(val_labels, val_preds)\n",
    "  val_ac = classification_report(val_labels, val_preds, output_dict=True)['macro avg']['f1-score']\n",
    "\n",
    "\n",
    "#   writer.add_scalar(\"Loss/val\", val_loss, epoch)\n",
    "#   writer.add_scalar(\"Acc/val\", val_ac, epoch)\n",
    "#   writer.add_scalar(\"macro-f1/val\", reports['macro avg']['f1-score'], epoch)\n",
    "  \n",
    "  return val_loss, val_ac\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {
    "id": "eC6cw0Ywqgb_"
   },
   "outputs": [],
   "source": [
    "\n",
    "def testing(model, dm):\n",
    "  model.eval()\n",
    "  test_running_loss = 0.0\n",
    "  counter = 0\n",
    "  test_preds = torch.tensor([], device=device)\n",
    "  test_labels = torch.tensor([], device = device)\n",
    "\n",
    "  soft_m = nn.Softmax(dim=-1)\n",
    "\n",
    "\n",
    "  with torch.no_grad():\n",
    "    for i, (texts, images, inputs, labels) in enumerate(dm.test_dataloader()):\n",
    "        counter += 1\n",
    "        labels = labels.to(device)\n",
    "\n",
    "        outputs = model(images, texts, inputs)\n",
    "\n",
    "        output_scores = soft_m(outputs)\n",
    "        predictions = torch.argmax(output_scores, dim=-1)\n",
    "\n",
    "        test_preds = torch.cat((test_preds, predictions), dim=0)\n",
    "        test_labels = torch.cat((test_labels, labels), dim=0)\n",
    "\n",
    "        # loss = criterion(outputs, labels)\n",
    "        # test_running_loss += loss.item()\n",
    "    \n",
    "  test_preds = test_preds.cpu().numpy()\n",
    "  test_labels = test_labels.cpu().numpy()\n",
    "       \n",
    "  print('test', classification_report(test_labels, test_preds))\n",
    "  print('test_ac', accuracy_score(test_labels, test_preds))\n",
    "  print('Confusion', confusion_matrix(test_labels, test_preds))\n",
    "  print('Macro_f1', classification_report(test_labels, test_preds, output_dict=True)['macro avg']['f1-score'])\n",
    "  \n",
    "  # test_loss = test_running_loss / counter\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {
    "id": "vbONbjzw0zLY"
   },
   "outputs": [],
   "source": [
    "# (t, i ,l) = next(iter(dm.train_dataloader()))\n",
    "# mo = Concatmodal()\n",
    "# ou = mo(i, t)\n",
    "# print(ou)\n",
    "# sf = nn.Softmax(dim=-1)\n",
    "# ou = sf(ou)\n",
    "# print(ou)\n",
    "# ou = torch.argmax(ou, dim=-1)\n",
    "# print(ou)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {
    "id": "_J90rwZ6OPkF"
   },
   "outputs": [],
   "source": [
    "# testing(mo, dm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {
    "id": "QGsSjernAIOG"
   },
   "outputs": [],
   "source": [
    "def save_checkpoint(state, filename='my_checkpoint_attention_seed_84.pth.tar'):\n",
    "    print('Saving....'+ filename)\n",
    "    torch.save(state, filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_checkpoint1(state, filename='my_checkpoint_attention_seed_84_f1.pth.tar'):\n",
    "    print('Saving....'+ filename)\n",
    "    torch.save(state, filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "J-I_Y6mlANvK",
    "outputId": "958dec5d-6e4f-48ec-de58-5a582d4481b7"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[0m\u001b[01;31marchive.zip\u001b[0m\r\n",
      "Attetion-Multimodal-Bin2.ipynb\r\n",
      "Attetion-Multimodal-Bin2-Moredata-Copy1.ipynb\r\n",
      "Attetion-Multimodal-Bin2-Moredata.ipynb\r\n",
      "Attetion-Multimodal-Bin.ipynb\r\n",
      "Attetion-Multimodal.ipynb\r\n",
      "Attetion-Multimodal-LateFusion-Copy1.ipynb\r\n",
      "Attetion-Multimodal-LateFusion.ipynb\r\n",
      "Attetion-Multimodal-seed25-1e-3-Copy1.ipynb\r\n",
      "Attetion-Multimodal-seed25-1e-3.ipynb\r\n",
      "Attetion-Multimodal-seed7.ipynb\r\n",
      "Attetion-Multimodal-Soccer.ipynb\r\n",
      "CNN.ipynb\r\n",
      "Inception-Attetion-Copy1.ipynb\r\n",
      "Inception-Attetion-Copy2.ipynb\r\n",
      "Inception-Attetion.ipynb\r\n",
      "Inception.ipynb\r\n",
      "Inception-VotingMultimodal.ipynb\r\n",
      "\u001b[01;31mlocal1_folder_temp.zip\u001b[0m\r\n",
      "\u001b[01;31mlocal1_folder.zip\u001b[0m\r\n",
      "\u001b[01;31mmountains.zip\u001b[0m\r\n",
      "Multimodal_newmodel.ipynb\r\n",
      "Multimodal_newmodel-weight.ipynb\r\n",
      "\u001b[01;31mmy_checkpoint_attention_seed_84_bin_img.pth.tar\u001b[0m\r\n",
      "\u001b[01;31mmy_checkpoint_attention_seed_84_f1_bin_img.pth.tar\u001b[0m\r\n",
      "\u001b[01;31mmy_checkpoint_attention_seed_84_f1.pth.tar\u001b[0m\r\n",
      "\u001b[01;31mmy_checkpoint_attention_seed_84.pth.tar\u001b[0m\r\n",
      "\u001b[01;31mmy_checkpoint_incept_attention_seed_84_1e-3_data_weight.pth.tar\u001b[0m\r\n",
      "\u001b[01;31mmy_checkpoint_incept_attention_seed_84_1e-3_drop02.pth.tar\u001b[0m\r\n",
      "\u001b[01;31mmy_checkpoint_incept_attention_seed_84_1e-3.pth.tar\u001b[0m\r\n",
      "\u001b[01;31mmy_checkpoint_incept_attention_seed_84_f1_1e-3_data_weight.pth.tar\u001b[0m\r\n",
      "\u001b[01;31mmy_checkpoint_incept_attention_seed_84_f1_1e-3_drop02.pth.tar\u001b[0m\r\n",
      "\u001b[01;31mmy_checkpoint_incept_attention_seed_84_f1_1e-3.pth.tar\u001b[0m\r\n",
      "\u001b[01;31mmy_checkpoint_incept_seed7_bt8_fc_2048_re512_re128_1e-3.pth.tar\u001b[0m\r\n",
      "\u001b[01;31mmy_checkpoint_incept_seed84_bt8_fc_2048_re512_re128_1e-3.pth.tar\u001b[0m\r\n",
      "\u001b[01;31mmy_checkpoint_incept_voting.pth.tar\u001b[0m\r\n",
      "\u001b[01;34mOUTPUT\u001b[0m/\r\n",
      "PhoBERT_foody.ipynb\r\n",
      "PhoBERT_foody_rebuild.ipynb\r\n",
      "Resnet.ipynb\r\n",
      "\u001b[01;34mruns\u001b[0m/\r\n",
      "\u001b[01;31msoccer.zip\u001b[0m\r\n",
      "Stack-MaxMultimodal.ipynb\r\n",
      "\u001b[01;34mvncorenlp\u001b[0m/\r\n",
      "VotingMultimodal-Dense.ipynb\r\n",
      "VotingMultimodal-fc2048-512-128.ipynb\r\n",
      "VotingMultimodal_newmodel-CNN.ipynb\r\n",
      "VotingMultimodal-offficial.ipynb\r\n"
     ]
    }
   ],
   "source": [
    "%ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {
    "id": "tUOcHfHQzPvb"
   },
   "outputs": [],
   "source": [
    "# model = Concatmodal()\n",
    "# total_params = sum(p.numel() for p in model.parameters())\n",
    "# print(f\"{total_params:,} total parameters.\")\n",
    "# total_trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "# print(f\"{total_trainable_params:,} training parameters.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {
    "id": "bakUYVqazYM6"
   },
   "outputs": [],
   "source": [
    "def freeze(model):  \n",
    "  for param in model.BERT.PhoBERT.parameters():\n",
    "    param.requires_grad = False\n",
    "  for param in model.incept.incept.parameters():\n",
    "    param.requires_grad = False\n",
    "  return model\n",
    "\n",
    "def unfreeze(model):  \n",
    "  for param in model.BERT.PhoBERT.parameters():\n",
    "    param.requires_grad = True\n",
    "  for param in model.incept.incept.parameters():\n",
    "    param.requires_grad = True\n",
    "    # print(param)\n",
    "  return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {
    "id": "83GHcVAuzodA"
   },
   "outputs": [],
   "source": [
    "# total_params = sum(p.numel() for p in model.parameters())\n",
    "# print(f\"{total_params:,} total parameters.\")\n",
    "# total_trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "# print(f\"{total_trainable_params:,} training parameters.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import gc\n",
    "# del model\n",
    "# gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at vinai/phobert-base were not used when initializing RobertaModel: ['lm_head.dense.weight', 'lm_head.layer_norm.weight', 'lm_head.dense.bias', 'lm_head.bias', 'lm_head.decoder.bias', 'lm_head.layer_norm.bias', 'lm_head.decoder.weight']\n",
      "- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "model = Concatmodal()\n",
    "model = freeze(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "160,473,802 total parameters.\n",
      "1,640,962 training parameters.\n"
     ]
    }
   ],
   "source": [
    "total_params = sum(p.numel() for p in model.parameters())\n",
    "print(f\"{total_params:,} total parameters.\")\n",
    "total_trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "print(f\"{total_trainable_params:,} training parameters.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Concatmodal(\n",
       "  (BERT): Bert_Lstm(\n",
       "    (PhoBERT): RobertaModel(\n",
       "      (embeddings): RobertaEmbeddings(\n",
       "        (word_embeddings): Embedding(64001, 768, padding_idx=1)\n",
       "        (position_embeddings): Embedding(258, 768, padding_idx=1)\n",
       "        (token_type_embeddings): Embedding(1, 768)\n",
       "        (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (encoder): RobertaEncoder(\n",
       "        (layer): ModuleList(\n",
       "          (0): RobertaLayer(\n",
       "            (attention): RobertaAttention(\n",
       "              (self): RobertaSelfAttention(\n",
       "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "              (output): RobertaSelfOutput(\n",
       "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "            )\n",
       "            (intermediate): RobertaIntermediate(\n",
       "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "              (intermediate_act_fn): GELUActivation()\n",
       "            )\n",
       "            (output): RobertaOutput(\n",
       "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (1): RobertaLayer(\n",
       "            (attention): RobertaAttention(\n",
       "              (self): RobertaSelfAttention(\n",
       "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "              (output): RobertaSelfOutput(\n",
       "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "            )\n",
       "            (intermediate): RobertaIntermediate(\n",
       "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "              (intermediate_act_fn): GELUActivation()\n",
       "            )\n",
       "            (output): RobertaOutput(\n",
       "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (2): RobertaLayer(\n",
       "            (attention): RobertaAttention(\n",
       "              (self): RobertaSelfAttention(\n",
       "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "              (output): RobertaSelfOutput(\n",
       "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "            )\n",
       "            (intermediate): RobertaIntermediate(\n",
       "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "              (intermediate_act_fn): GELUActivation()\n",
       "            )\n",
       "            (output): RobertaOutput(\n",
       "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (3): RobertaLayer(\n",
       "            (attention): RobertaAttention(\n",
       "              (self): RobertaSelfAttention(\n",
       "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "              (output): RobertaSelfOutput(\n",
       "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "            )\n",
       "            (intermediate): RobertaIntermediate(\n",
       "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "              (intermediate_act_fn): GELUActivation()\n",
       "            )\n",
       "            (output): RobertaOutput(\n",
       "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (4): RobertaLayer(\n",
       "            (attention): RobertaAttention(\n",
       "              (self): RobertaSelfAttention(\n",
       "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "              (output): RobertaSelfOutput(\n",
       "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "            )\n",
       "            (intermediate): RobertaIntermediate(\n",
       "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "              (intermediate_act_fn): GELUActivation()\n",
       "            )\n",
       "            (output): RobertaOutput(\n",
       "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (5): RobertaLayer(\n",
       "            (attention): RobertaAttention(\n",
       "              (self): RobertaSelfAttention(\n",
       "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "              (output): RobertaSelfOutput(\n",
       "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "            )\n",
       "            (intermediate): RobertaIntermediate(\n",
       "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "              (intermediate_act_fn): GELUActivation()\n",
       "            )\n",
       "            (output): RobertaOutput(\n",
       "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (6): RobertaLayer(\n",
       "            (attention): RobertaAttention(\n",
       "              (self): RobertaSelfAttention(\n",
       "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "              (output): RobertaSelfOutput(\n",
       "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "            )\n",
       "            (intermediate): RobertaIntermediate(\n",
       "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "              (intermediate_act_fn): GELUActivation()\n",
       "            )\n",
       "            (output): RobertaOutput(\n",
       "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (7): RobertaLayer(\n",
       "            (attention): RobertaAttention(\n",
       "              (self): RobertaSelfAttention(\n",
       "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "              (output): RobertaSelfOutput(\n",
       "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "            )\n",
       "            (intermediate): RobertaIntermediate(\n",
       "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "              (intermediate_act_fn): GELUActivation()\n",
       "            )\n",
       "            (output): RobertaOutput(\n",
       "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (8): RobertaLayer(\n",
       "            (attention): RobertaAttention(\n",
       "              (self): RobertaSelfAttention(\n",
       "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "              (output): RobertaSelfOutput(\n",
       "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "            )\n",
       "            (intermediate): RobertaIntermediate(\n",
       "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "              (intermediate_act_fn): GELUActivation()\n",
       "            )\n",
       "            (output): RobertaOutput(\n",
       "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (9): RobertaLayer(\n",
       "            (attention): RobertaAttention(\n",
       "              (self): RobertaSelfAttention(\n",
       "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "              (output): RobertaSelfOutput(\n",
       "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "            )\n",
       "            (intermediate): RobertaIntermediate(\n",
       "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "              (intermediate_act_fn): GELUActivation()\n",
       "            )\n",
       "            (output): RobertaOutput(\n",
       "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (10): RobertaLayer(\n",
       "            (attention): RobertaAttention(\n",
       "              (self): RobertaSelfAttention(\n",
       "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "              (output): RobertaSelfOutput(\n",
       "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "            )\n",
       "            (intermediate): RobertaIntermediate(\n",
       "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "              (intermediate_act_fn): GELUActivation()\n",
       "            )\n",
       "            (output): RobertaOutput(\n",
       "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (11): RobertaLayer(\n",
       "            (attention): RobertaAttention(\n",
       "              (self): RobertaSelfAttention(\n",
       "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "              (output): RobertaSelfOutput(\n",
       "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "            )\n",
       "            (intermediate): RobertaIntermediate(\n",
       "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "              (intermediate_act_fn): GELUActivation()\n",
       "            )\n",
       "            (output): RobertaOutput(\n",
       "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (pooler): RobertaPooler(\n",
       "        (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (activation): Tanh()\n",
       "      )\n",
       "    )\n",
       "    (lstm): LSTM(768, 128, batch_first=True)\n",
       "  )\n",
       "  (incept): Inception(\n",
       "    (incept): Inception3(\n",
       "      (Conv2d_1a_3x3): BasicConv2d(\n",
       "        (conv): Conv2d(3, 32, kernel_size=(3, 3), stride=(2, 2), bias=False)\n",
       "        (bn): BatchNorm2d(32, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "      (Conv2d_2a_3x3): BasicConv2d(\n",
       "        (conv): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), bias=False)\n",
       "        (bn): BatchNorm2d(32, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "      (Conv2d_2b_3x3): BasicConv2d(\n",
       "        (conv): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn): BatchNorm2d(64, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "      (maxpool1): MaxPool2d(kernel_size=3, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "      (Conv2d_3b_1x1): BasicConv2d(\n",
       "        (conv): Conv2d(64, 80, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn): BatchNorm2d(80, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "      (Conv2d_4a_3x3): BasicConv2d(\n",
       "        (conv): Conv2d(80, 192, kernel_size=(3, 3), stride=(1, 1), bias=False)\n",
       "        (bn): BatchNorm2d(192, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "      (maxpool2): MaxPool2d(kernel_size=3, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "      (Mixed_5b): InceptionA(\n",
       "        (branch1x1): BasicConv2d(\n",
       "          (conv): Conv2d(192, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn): BatchNorm2d(64, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "        (branch5x5_1): BasicConv2d(\n",
       "          (conv): Conv2d(192, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn): BatchNorm2d(48, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "        (branch5x5_2): BasicConv2d(\n",
       "          (conv): Conv2d(48, 64, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), bias=False)\n",
       "          (bn): BatchNorm2d(64, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "        (branch3x3dbl_1): BasicConv2d(\n",
       "          (conv): Conv2d(192, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn): BatchNorm2d(64, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "        (branch3x3dbl_2): BasicConv2d(\n",
       "          (conv): Conv2d(64, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn): BatchNorm2d(96, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "        (branch3x3dbl_3): BasicConv2d(\n",
       "          (conv): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn): BatchNorm2d(96, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "        (branch_pool): BasicConv2d(\n",
       "          (conv): Conv2d(192, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn): BatchNorm2d(32, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "      )\n",
       "      (Mixed_5c): InceptionA(\n",
       "        (branch1x1): BasicConv2d(\n",
       "          (conv): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn): BatchNorm2d(64, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "        (branch5x5_1): BasicConv2d(\n",
       "          (conv): Conv2d(256, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn): BatchNorm2d(48, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "        (branch5x5_2): BasicConv2d(\n",
       "          (conv): Conv2d(48, 64, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), bias=False)\n",
       "          (bn): BatchNorm2d(64, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "        (branch3x3dbl_1): BasicConv2d(\n",
       "          (conv): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn): BatchNorm2d(64, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "        (branch3x3dbl_2): BasicConv2d(\n",
       "          (conv): Conv2d(64, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn): BatchNorm2d(96, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "        (branch3x3dbl_3): BasicConv2d(\n",
       "          (conv): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn): BatchNorm2d(96, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "        (branch_pool): BasicConv2d(\n",
       "          (conv): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn): BatchNorm2d(64, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "      )\n",
       "      (Mixed_5d): InceptionA(\n",
       "        (branch1x1): BasicConv2d(\n",
       "          (conv): Conv2d(288, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn): BatchNorm2d(64, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "        (branch5x5_1): BasicConv2d(\n",
       "          (conv): Conv2d(288, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn): BatchNorm2d(48, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "        (branch5x5_2): BasicConv2d(\n",
       "          (conv): Conv2d(48, 64, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), bias=False)\n",
       "          (bn): BatchNorm2d(64, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "        (branch3x3dbl_1): BasicConv2d(\n",
       "          (conv): Conv2d(288, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn): BatchNorm2d(64, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "        (branch3x3dbl_2): BasicConv2d(\n",
       "          (conv): Conv2d(64, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn): BatchNorm2d(96, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "        (branch3x3dbl_3): BasicConv2d(\n",
       "          (conv): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn): BatchNorm2d(96, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "        (branch_pool): BasicConv2d(\n",
       "          (conv): Conv2d(288, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn): BatchNorm2d(64, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "      )\n",
       "      (Mixed_6a): InceptionB(\n",
       "        (branch3x3): BasicConv2d(\n",
       "          (conv): Conv2d(288, 384, kernel_size=(3, 3), stride=(2, 2), bias=False)\n",
       "          (bn): BatchNorm2d(384, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "        (branch3x3dbl_1): BasicConv2d(\n",
       "          (conv): Conv2d(288, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn): BatchNorm2d(64, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "        (branch3x3dbl_2): BasicConv2d(\n",
       "          (conv): Conv2d(64, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn): BatchNorm2d(96, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "        (branch3x3dbl_3): BasicConv2d(\n",
       "          (conv): Conv2d(96, 96, kernel_size=(3, 3), stride=(2, 2), bias=False)\n",
       "          (bn): BatchNorm2d(96, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "      )\n",
       "      (Mixed_6b): InceptionC(\n",
       "        (branch1x1): BasicConv2d(\n",
       "          (conv): Conv2d(768, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn): BatchNorm2d(192, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "        (branch7x7_1): BasicConv2d(\n",
       "          (conv): Conv2d(768, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn): BatchNorm2d(128, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "        (branch7x7_2): BasicConv2d(\n",
       "          (conv): Conv2d(128, 128, kernel_size=(1, 7), stride=(1, 1), padding=(0, 3), bias=False)\n",
       "          (bn): BatchNorm2d(128, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "        (branch7x7_3): BasicConv2d(\n",
       "          (conv): Conv2d(128, 192, kernel_size=(7, 1), stride=(1, 1), padding=(3, 0), bias=False)\n",
       "          (bn): BatchNorm2d(192, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "        (branch7x7dbl_1): BasicConv2d(\n",
       "          (conv): Conv2d(768, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn): BatchNorm2d(128, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "        (branch7x7dbl_2): BasicConv2d(\n",
       "          (conv): Conv2d(128, 128, kernel_size=(7, 1), stride=(1, 1), padding=(3, 0), bias=False)\n",
       "          (bn): BatchNorm2d(128, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "        (branch7x7dbl_3): BasicConv2d(\n",
       "          (conv): Conv2d(128, 128, kernel_size=(1, 7), stride=(1, 1), padding=(0, 3), bias=False)\n",
       "          (bn): BatchNorm2d(128, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "        (branch7x7dbl_4): BasicConv2d(\n",
       "          (conv): Conv2d(128, 128, kernel_size=(7, 1), stride=(1, 1), padding=(3, 0), bias=False)\n",
       "          (bn): BatchNorm2d(128, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "        (branch7x7dbl_5): BasicConv2d(\n",
       "          (conv): Conv2d(128, 192, kernel_size=(1, 7), stride=(1, 1), padding=(0, 3), bias=False)\n",
       "          (bn): BatchNorm2d(192, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "        (branch_pool): BasicConv2d(\n",
       "          (conv): Conv2d(768, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn): BatchNorm2d(192, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "      )\n",
       "      (Mixed_6c): InceptionC(\n",
       "        (branch1x1): BasicConv2d(\n",
       "          (conv): Conv2d(768, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn): BatchNorm2d(192, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "        (branch7x7_1): BasicConv2d(\n",
       "          (conv): Conv2d(768, 160, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn): BatchNorm2d(160, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "        (branch7x7_2): BasicConv2d(\n",
       "          (conv): Conv2d(160, 160, kernel_size=(1, 7), stride=(1, 1), padding=(0, 3), bias=False)\n",
       "          (bn): BatchNorm2d(160, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "        (branch7x7_3): BasicConv2d(\n",
       "          (conv): Conv2d(160, 192, kernel_size=(7, 1), stride=(1, 1), padding=(3, 0), bias=False)\n",
       "          (bn): BatchNorm2d(192, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "        (branch7x7dbl_1): BasicConv2d(\n",
       "          (conv): Conv2d(768, 160, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn): BatchNorm2d(160, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "        (branch7x7dbl_2): BasicConv2d(\n",
       "          (conv): Conv2d(160, 160, kernel_size=(7, 1), stride=(1, 1), padding=(3, 0), bias=False)\n",
       "          (bn): BatchNorm2d(160, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "        (branch7x7dbl_3): BasicConv2d(\n",
       "          (conv): Conv2d(160, 160, kernel_size=(1, 7), stride=(1, 1), padding=(0, 3), bias=False)\n",
       "          (bn): BatchNorm2d(160, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "        (branch7x7dbl_4): BasicConv2d(\n",
       "          (conv): Conv2d(160, 160, kernel_size=(7, 1), stride=(1, 1), padding=(3, 0), bias=False)\n",
       "          (bn): BatchNorm2d(160, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "        (branch7x7dbl_5): BasicConv2d(\n",
       "          (conv): Conv2d(160, 192, kernel_size=(1, 7), stride=(1, 1), padding=(0, 3), bias=False)\n",
       "          (bn): BatchNorm2d(192, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "        (branch_pool): BasicConv2d(\n",
       "          (conv): Conv2d(768, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn): BatchNorm2d(192, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "      )\n",
       "      (Mixed_6d): InceptionC(\n",
       "        (branch1x1): BasicConv2d(\n",
       "          (conv): Conv2d(768, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn): BatchNorm2d(192, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "        (branch7x7_1): BasicConv2d(\n",
       "          (conv): Conv2d(768, 160, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn): BatchNorm2d(160, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "        (branch7x7_2): BasicConv2d(\n",
       "          (conv): Conv2d(160, 160, kernel_size=(1, 7), stride=(1, 1), padding=(0, 3), bias=False)\n",
       "          (bn): BatchNorm2d(160, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "        (branch7x7_3): BasicConv2d(\n",
       "          (conv): Conv2d(160, 192, kernel_size=(7, 1), stride=(1, 1), padding=(3, 0), bias=False)\n",
       "          (bn): BatchNorm2d(192, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "        (branch7x7dbl_1): BasicConv2d(\n",
       "          (conv): Conv2d(768, 160, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn): BatchNorm2d(160, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "        (branch7x7dbl_2): BasicConv2d(\n",
       "          (conv): Conv2d(160, 160, kernel_size=(7, 1), stride=(1, 1), padding=(3, 0), bias=False)\n",
       "          (bn): BatchNorm2d(160, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "        (branch7x7dbl_3): BasicConv2d(\n",
       "          (conv): Conv2d(160, 160, kernel_size=(1, 7), stride=(1, 1), padding=(0, 3), bias=False)\n",
       "          (bn): BatchNorm2d(160, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "        (branch7x7dbl_4): BasicConv2d(\n",
       "          (conv): Conv2d(160, 160, kernel_size=(7, 1), stride=(1, 1), padding=(3, 0), bias=False)\n",
       "          (bn): BatchNorm2d(160, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "        (branch7x7dbl_5): BasicConv2d(\n",
       "          (conv): Conv2d(160, 192, kernel_size=(1, 7), stride=(1, 1), padding=(0, 3), bias=False)\n",
       "          (bn): BatchNorm2d(192, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "        (branch_pool): BasicConv2d(\n",
       "          (conv): Conv2d(768, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn): BatchNorm2d(192, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "      )\n",
       "      (Mixed_6e): InceptionC(\n",
       "        (branch1x1): BasicConv2d(\n",
       "          (conv): Conv2d(768, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn): BatchNorm2d(192, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "        (branch7x7_1): BasicConv2d(\n",
       "          (conv): Conv2d(768, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn): BatchNorm2d(192, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "        (branch7x7_2): BasicConv2d(\n",
       "          (conv): Conv2d(192, 192, kernel_size=(1, 7), stride=(1, 1), padding=(0, 3), bias=False)\n",
       "          (bn): BatchNorm2d(192, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "        (branch7x7_3): BasicConv2d(\n",
       "          (conv): Conv2d(192, 192, kernel_size=(7, 1), stride=(1, 1), padding=(3, 0), bias=False)\n",
       "          (bn): BatchNorm2d(192, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "        (branch7x7dbl_1): BasicConv2d(\n",
       "          (conv): Conv2d(768, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn): BatchNorm2d(192, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "        (branch7x7dbl_2): BasicConv2d(\n",
       "          (conv): Conv2d(192, 192, kernel_size=(7, 1), stride=(1, 1), padding=(3, 0), bias=False)\n",
       "          (bn): BatchNorm2d(192, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "        (branch7x7dbl_3): BasicConv2d(\n",
       "          (conv): Conv2d(192, 192, kernel_size=(1, 7), stride=(1, 1), padding=(0, 3), bias=False)\n",
       "          (bn): BatchNorm2d(192, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "        (branch7x7dbl_4): BasicConv2d(\n",
       "          (conv): Conv2d(192, 192, kernel_size=(7, 1), stride=(1, 1), padding=(3, 0), bias=False)\n",
       "          (bn): BatchNorm2d(192, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "        (branch7x7dbl_5): BasicConv2d(\n",
       "          (conv): Conv2d(192, 192, kernel_size=(1, 7), stride=(1, 1), padding=(0, 3), bias=False)\n",
       "          (bn): BatchNorm2d(192, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "        (branch_pool): BasicConv2d(\n",
       "          (conv): Conv2d(768, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn): BatchNorm2d(192, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "      )\n",
       "      (AuxLogits): None\n",
       "      (Mixed_7a): InceptionD(\n",
       "        (branch3x3_1): BasicConv2d(\n",
       "          (conv): Conv2d(768, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn): BatchNorm2d(192, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "        (branch3x3_2): BasicConv2d(\n",
       "          (conv): Conv2d(192, 320, kernel_size=(3, 3), stride=(2, 2), bias=False)\n",
       "          (bn): BatchNorm2d(320, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "        (branch7x7x3_1): BasicConv2d(\n",
       "          (conv): Conv2d(768, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn): BatchNorm2d(192, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "        (branch7x7x3_2): BasicConv2d(\n",
       "          (conv): Conv2d(192, 192, kernel_size=(1, 7), stride=(1, 1), padding=(0, 3), bias=False)\n",
       "          (bn): BatchNorm2d(192, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "        (branch7x7x3_3): BasicConv2d(\n",
       "          (conv): Conv2d(192, 192, kernel_size=(7, 1), stride=(1, 1), padding=(3, 0), bias=False)\n",
       "          (bn): BatchNorm2d(192, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "        (branch7x7x3_4): BasicConv2d(\n",
       "          (conv): Conv2d(192, 192, kernel_size=(3, 3), stride=(2, 2), bias=False)\n",
       "          (bn): BatchNorm2d(192, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "      )\n",
       "      (Mixed_7b): InceptionE(\n",
       "        (branch1x1): BasicConv2d(\n",
       "          (conv): Conv2d(1280, 320, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn): BatchNorm2d(320, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "        (branch3x3_1): BasicConv2d(\n",
       "          (conv): Conv2d(1280, 384, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn): BatchNorm2d(384, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "        (branch3x3_2a): BasicConv2d(\n",
       "          (conv): Conv2d(384, 384, kernel_size=(1, 3), stride=(1, 1), padding=(0, 1), bias=False)\n",
       "          (bn): BatchNorm2d(384, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "        (branch3x3_2b): BasicConv2d(\n",
       "          (conv): Conv2d(384, 384, kernel_size=(3, 1), stride=(1, 1), padding=(1, 0), bias=False)\n",
       "          (bn): BatchNorm2d(384, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "        (branch3x3dbl_1): BasicConv2d(\n",
       "          (conv): Conv2d(1280, 448, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn): BatchNorm2d(448, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "        (branch3x3dbl_2): BasicConv2d(\n",
       "          (conv): Conv2d(448, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn): BatchNorm2d(384, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "        (branch3x3dbl_3a): BasicConv2d(\n",
       "          (conv): Conv2d(384, 384, kernel_size=(1, 3), stride=(1, 1), padding=(0, 1), bias=False)\n",
       "          (bn): BatchNorm2d(384, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "        (branch3x3dbl_3b): BasicConv2d(\n",
       "          (conv): Conv2d(384, 384, kernel_size=(3, 1), stride=(1, 1), padding=(1, 0), bias=False)\n",
       "          (bn): BatchNorm2d(384, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "        (branch_pool): BasicConv2d(\n",
       "          (conv): Conv2d(1280, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn): BatchNorm2d(192, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "      )\n",
       "      (Mixed_7c): InceptionE(\n",
       "        (branch1x1): BasicConv2d(\n",
       "          (conv): Conv2d(2048, 320, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn): BatchNorm2d(320, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "        (branch3x3_1): BasicConv2d(\n",
       "          (conv): Conv2d(2048, 384, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn): BatchNorm2d(384, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "        (branch3x3_2a): BasicConv2d(\n",
       "          (conv): Conv2d(384, 384, kernel_size=(1, 3), stride=(1, 1), padding=(0, 1), bias=False)\n",
       "          (bn): BatchNorm2d(384, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "        (branch3x3_2b): BasicConv2d(\n",
       "          (conv): Conv2d(384, 384, kernel_size=(3, 1), stride=(1, 1), padding=(1, 0), bias=False)\n",
       "          (bn): BatchNorm2d(384, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "        (branch3x3dbl_1): BasicConv2d(\n",
       "          (conv): Conv2d(2048, 448, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn): BatchNorm2d(448, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "        (branch3x3dbl_2): BasicConv2d(\n",
       "          (conv): Conv2d(448, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn): BatchNorm2d(384, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "        (branch3x3dbl_3a): BasicConv2d(\n",
       "          (conv): Conv2d(384, 384, kernel_size=(1, 3), stride=(1, 1), padding=(0, 1), bias=False)\n",
       "          (bn): BatchNorm2d(384, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "        (branch3x3dbl_3b): BasicConv2d(\n",
       "          (conv): Conv2d(384, 384, kernel_size=(3, 1), stride=(1, 1), padding=(1, 0), bias=False)\n",
       "          (bn): BatchNorm2d(384, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "        (branch_pool): BasicConv2d(\n",
       "          (conv): Conv2d(2048, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn): BatchNorm2d(192, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "      )\n",
       "      (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))\n",
       "      (dropout): Dropout(p=0.5, inplace=False)\n",
       "      (fc): Linear(in_features=2048, out_features=1000, bias=True)\n",
       "    )\n",
       "    (incept_sequen): Sequential(\n",
       "      (0): BasicConv2d(\n",
       "        (conv): Conv2d(3, 32, kernel_size=(3, 3), stride=(2, 2), bias=False)\n",
       "        (bn): BatchNorm2d(32, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "      (1): BasicConv2d(\n",
       "        (conv): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), bias=False)\n",
       "        (bn): BatchNorm2d(32, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "      (2): BasicConv2d(\n",
       "        (conv): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn): BatchNorm2d(64, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "      (3): MaxPool2d(kernel_size=3, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "      (4): BasicConv2d(\n",
       "        (conv): Conv2d(64, 80, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn): BatchNorm2d(80, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "      (5): BasicConv2d(\n",
       "        (conv): Conv2d(80, 192, kernel_size=(3, 3), stride=(1, 1), bias=False)\n",
       "        (bn): BatchNorm2d(192, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "      (6): MaxPool2d(kernel_size=3, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "      (7): InceptionA(\n",
       "        (branch1x1): BasicConv2d(\n",
       "          (conv): Conv2d(192, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn): BatchNorm2d(64, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "        (branch5x5_1): BasicConv2d(\n",
       "          (conv): Conv2d(192, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn): BatchNorm2d(48, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "        (branch5x5_2): BasicConv2d(\n",
       "          (conv): Conv2d(48, 64, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), bias=False)\n",
       "          (bn): BatchNorm2d(64, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "        (branch3x3dbl_1): BasicConv2d(\n",
       "          (conv): Conv2d(192, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn): BatchNorm2d(64, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "        (branch3x3dbl_2): BasicConv2d(\n",
       "          (conv): Conv2d(64, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn): BatchNorm2d(96, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "        (branch3x3dbl_3): BasicConv2d(\n",
       "          (conv): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn): BatchNorm2d(96, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "        (branch_pool): BasicConv2d(\n",
       "          (conv): Conv2d(192, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn): BatchNorm2d(32, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "      )\n",
       "      (8): InceptionA(\n",
       "        (branch1x1): BasicConv2d(\n",
       "          (conv): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn): BatchNorm2d(64, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "        (branch5x5_1): BasicConv2d(\n",
       "          (conv): Conv2d(256, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn): BatchNorm2d(48, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "        (branch5x5_2): BasicConv2d(\n",
       "          (conv): Conv2d(48, 64, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), bias=False)\n",
       "          (bn): BatchNorm2d(64, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "        (branch3x3dbl_1): BasicConv2d(\n",
       "          (conv): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn): BatchNorm2d(64, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "        (branch3x3dbl_2): BasicConv2d(\n",
       "          (conv): Conv2d(64, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn): BatchNorm2d(96, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "        (branch3x3dbl_3): BasicConv2d(\n",
       "          (conv): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn): BatchNorm2d(96, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "        (branch_pool): BasicConv2d(\n",
       "          (conv): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn): BatchNorm2d(64, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "      )\n",
       "      (9): InceptionA(\n",
       "        (branch1x1): BasicConv2d(\n",
       "          (conv): Conv2d(288, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn): BatchNorm2d(64, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "        (branch5x5_1): BasicConv2d(\n",
       "          (conv): Conv2d(288, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn): BatchNorm2d(48, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "        (branch5x5_2): BasicConv2d(\n",
       "          (conv): Conv2d(48, 64, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), bias=False)\n",
       "          (bn): BatchNorm2d(64, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "        (branch3x3dbl_1): BasicConv2d(\n",
       "          (conv): Conv2d(288, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn): BatchNorm2d(64, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "        (branch3x3dbl_2): BasicConv2d(\n",
       "          (conv): Conv2d(64, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn): BatchNorm2d(96, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "        (branch3x3dbl_3): BasicConv2d(\n",
       "          (conv): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn): BatchNorm2d(96, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "        (branch_pool): BasicConv2d(\n",
       "          (conv): Conv2d(288, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn): BatchNorm2d(64, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "      )\n",
       "      (10): InceptionB(\n",
       "        (branch3x3): BasicConv2d(\n",
       "          (conv): Conv2d(288, 384, kernel_size=(3, 3), stride=(2, 2), bias=False)\n",
       "          (bn): BatchNorm2d(384, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "        (branch3x3dbl_1): BasicConv2d(\n",
       "          (conv): Conv2d(288, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn): BatchNorm2d(64, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "        (branch3x3dbl_2): BasicConv2d(\n",
       "          (conv): Conv2d(64, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn): BatchNorm2d(96, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "        (branch3x3dbl_3): BasicConv2d(\n",
       "          (conv): Conv2d(96, 96, kernel_size=(3, 3), stride=(2, 2), bias=False)\n",
       "          (bn): BatchNorm2d(96, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "      )\n",
       "      (11): InceptionC(\n",
       "        (branch1x1): BasicConv2d(\n",
       "          (conv): Conv2d(768, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn): BatchNorm2d(192, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "        (branch7x7_1): BasicConv2d(\n",
       "          (conv): Conv2d(768, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn): BatchNorm2d(128, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "        (branch7x7_2): BasicConv2d(\n",
       "          (conv): Conv2d(128, 128, kernel_size=(1, 7), stride=(1, 1), padding=(0, 3), bias=False)\n",
       "          (bn): BatchNorm2d(128, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "        (branch7x7_3): BasicConv2d(\n",
       "          (conv): Conv2d(128, 192, kernel_size=(7, 1), stride=(1, 1), padding=(3, 0), bias=False)\n",
       "          (bn): BatchNorm2d(192, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "        (branch7x7dbl_1): BasicConv2d(\n",
       "          (conv): Conv2d(768, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn): BatchNorm2d(128, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "        (branch7x7dbl_2): BasicConv2d(\n",
       "          (conv): Conv2d(128, 128, kernel_size=(7, 1), stride=(1, 1), padding=(3, 0), bias=False)\n",
       "          (bn): BatchNorm2d(128, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "        (branch7x7dbl_3): BasicConv2d(\n",
       "          (conv): Conv2d(128, 128, kernel_size=(1, 7), stride=(1, 1), padding=(0, 3), bias=False)\n",
       "          (bn): BatchNorm2d(128, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "        (branch7x7dbl_4): BasicConv2d(\n",
       "          (conv): Conv2d(128, 128, kernel_size=(7, 1), stride=(1, 1), padding=(3, 0), bias=False)\n",
       "          (bn): BatchNorm2d(128, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "        (branch7x7dbl_5): BasicConv2d(\n",
       "          (conv): Conv2d(128, 192, kernel_size=(1, 7), stride=(1, 1), padding=(0, 3), bias=False)\n",
       "          (bn): BatchNorm2d(192, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "        (branch_pool): BasicConv2d(\n",
       "          (conv): Conv2d(768, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn): BatchNorm2d(192, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "      )\n",
       "      (12): InceptionC(\n",
       "        (branch1x1): BasicConv2d(\n",
       "          (conv): Conv2d(768, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn): BatchNorm2d(192, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "        (branch7x7_1): BasicConv2d(\n",
       "          (conv): Conv2d(768, 160, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn): BatchNorm2d(160, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "        (branch7x7_2): BasicConv2d(\n",
       "          (conv): Conv2d(160, 160, kernel_size=(1, 7), stride=(1, 1), padding=(0, 3), bias=False)\n",
       "          (bn): BatchNorm2d(160, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "        (branch7x7_3): BasicConv2d(\n",
       "          (conv): Conv2d(160, 192, kernel_size=(7, 1), stride=(1, 1), padding=(3, 0), bias=False)\n",
       "          (bn): BatchNorm2d(192, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "        (branch7x7dbl_1): BasicConv2d(\n",
       "          (conv): Conv2d(768, 160, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn): BatchNorm2d(160, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "        (branch7x7dbl_2): BasicConv2d(\n",
       "          (conv): Conv2d(160, 160, kernel_size=(7, 1), stride=(1, 1), padding=(3, 0), bias=False)\n",
       "          (bn): BatchNorm2d(160, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "        (branch7x7dbl_3): BasicConv2d(\n",
       "          (conv): Conv2d(160, 160, kernel_size=(1, 7), stride=(1, 1), padding=(0, 3), bias=False)\n",
       "          (bn): BatchNorm2d(160, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "        (branch7x7dbl_4): BasicConv2d(\n",
       "          (conv): Conv2d(160, 160, kernel_size=(7, 1), stride=(1, 1), padding=(3, 0), bias=False)\n",
       "          (bn): BatchNorm2d(160, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "        (branch7x7dbl_5): BasicConv2d(\n",
       "          (conv): Conv2d(160, 192, kernel_size=(1, 7), stride=(1, 1), padding=(0, 3), bias=False)\n",
       "          (bn): BatchNorm2d(192, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "        (branch_pool): BasicConv2d(\n",
       "          (conv): Conv2d(768, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn): BatchNorm2d(192, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "      )\n",
       "      (13): InceptionC(\n",
       "        (branch1x1): BasicConv2d(\n",
       "          (conv): Conv2d(768, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn): BatchNorm2d(192, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "        (branch7x7_1): BasicConv2d(\n",
       "          (conv): Conv2d(768, 160, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn): BatchNorm2d(160, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "        (branch7x7_2): BasicConv2d(\n",
       "          (conv): Conv2d(160, 160, kernel_size=(1, 7), stride=(1, 1), padding=(0, 3), bias=False)\n",
       "          (bn): BatchNorm2d(160, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "        (branch7x7_3): BasicConv2d(\n",
       "          (conv): Conv2d(160, 192, kernel_size=(7, 1), stride=(1, 1), padding=(3, 0), bias=False)\n",
       "          (bn): BatchNorm2d(192, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "        (branch7x7dbl_1): BasicConv2d(\n",
       "          (conv): Conv2d(768, 160, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn): BatchNorm2d(160, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "        (branch7x7dbl_2): BasicConv2d(\n",
       "          (conv): Conv2d(160, 160, kernel_size=(7, 1), stride=(1, 1), padding=(3, 0), bias=False)\n",
       "          (bn): BatchNorm2d(160, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "        (branch7x7dbl_3): BasicConv2d(\n",
       "          (conv): Conv2d(160, 160, kernel_size=(1, 7), stride=(1, 1), padding=(0, 3), bias=False)\n",
       "          (bn): BatchNorm2d(160, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "        (branch7x7dbl_4): BasicConv2d(\n",
       "          (conv): Conv2d(160, 160, kernel_size=(7, 1), stride=(1, 1), padding=(3, 0), bias=False)\n",
       "          (bn): BatchNorm2d(160, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "        (branch7x7dbl_5): BasicConv2d(\n",
       "          (conv): Conv2d(160, 192, kernel_size=(1, 7), stride=(1, 1), padding=(0, 3), bias=False)\n",
       "          (bn): BatchNorm2d(192, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "        (branch_pool): BasicConv2d(\n",
       "          (conv): Conv2d(768, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn): BatchNorm2d(192, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "      )\n",
       "      (14): InceptionC(\n",
       "        (branch1x1): BasicConv2d(\n",
       "          (conv): Conv2d(768, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn): BatchNorm2d(192, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "        (branch7x7_1): BasicConv2d(\n",
       "          (conv): Conv2d(768, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn): BatchNorm2d(192, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "        (branch7x7_2): BasicConv2d(\n",
       "          (conv): Conv2d(192, 192, kernel_size=(1, 7), stride=(1, 1), padding=(0, 3), bias=False)\n",
       "          (bn): BatchNorm2d(192, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "        (branch7x7_3): BasicConv2d(\n",
       "          (conv): Conv2d(192, 192, kernel_size=(7, 1), stride=(1, 1), padding=(3, 0), bias=False)\n",
       "          (bn): BatchNorm2d(192, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "        (branch7x7dbl_1): BasicConv2d(\n",
       "          (conv): Conv2d(768, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn): BatchNorm2d(192, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "        (branch7x7dbl_2): BasicConv2d(\n",
       "          (conv): Conv2d(192, 192, kernel_size=(7, 1), stride=(1, 1), padding=(3, 0), bias=False)\n",
       "          (bn): BatchNorm2d(192, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "        (branch7x7dbl_3): BasicConv2d(\n",
       "          (conv): Conv2d(192, 192, kernel_size=(1, 7), stride=(1, 1), padding=(0, 3), bias=False)\n",
       "          (bn): BatchNorm2d(192, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "        (branch7x7dbl_4): BasicConv2d(\n",
       "          (conv): Conv2d(192, 192, kernel_size=(7, 1), stride=(1, 1), padding=(3, 0), bias=False)\n",
       "          (bn): BatchNorm2d(192, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "        (branch7x7dbl_5): BasicConv2d(\n",
       "          (conv): Conv2d(192, 192, kernel_size=(1, 7), stride=(1, 1), padding=(0, 3), bias=False)\n",
       "          (bn): BatchNorm2d(192, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "        (branch_pool): BasicConv2d(\n",
       "          (conv): Conv2d(768, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn): BatchNorm2d(192, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "      )\n",
       "      (15): InceptionD(\n",
       "        (branch3x3_1): BasicConv2d(\n",
       "          (conv): Conv2d(768, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn): BatchNorm2d(192, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "        (branch3x3_2): BasicConv2d(\n",
       "          (conv): Conv2d(192, 320, kernel_size=(3, 3), stride=(2, 2), bias=False)\n",
       "          (bn): BatchNorm2d(320, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "        (branch7x7x3_1): BasicConv2d(\n",
       "          (conv): Conv2d(768, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn): BatchNorm2d(192, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "        (branch7x7x3_2): BasicConv2d(\n",
       "          (conv): Conv2d(192, 192, kernel_size=(1, 7), stride=(1, 1), padding=(0, 3), bias=False)\n",
       "          (bn): BatchNorm2d(192, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "        (branch7x7x3_3): BasicConv2d(\n",
       "          (conv): Conv2d(192, 192, kernel_size=(7, 1), stride=(1, 1), padding=(3, 0), bias=False)\n",
       "          (bn): BatchNorm2d(192, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "        (branch7x7x3_4): BasicConv2d(\n",
       "          (conv): Conv2d(192, 192, kernel_size=(3, 3), stride=(2, 2), bias=False)\n",
       "          (bn): BatchNorm2d(192, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "      )\n",
       "      (16): InceptionE(\n",
       "        (branch1x1): BasicConv2d(\n",
       "          (conv): Conv2d(1280, 320, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn): BatchNorm2d(320, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "        (branch3x3_1): BasicConv2d(\n",
       "          (conv): Conv2d(1280, 384, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn): BatchNorm2d(384, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "        (branch3x3_2a): BasicConv2d(\n",
       "          (conv): Conv2d(384, 384, kernel_size=(1, 3), stride=(1, 1), padding=(0, 1), bias=False)\n",
       "          (bn): BatchNorm2d(384, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "        (branch3x3_2b): BasicConv2d(\n",
       "          (conv): Conv2d(384, 384, kernel_size=(3, 1), stride=(1, 1), padding=(1, 0), bias=False)\n",
       "          (bn): BatchNorm2d(384, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "        (branch3x3dbl_1): BasicConv2d(\n",
       "          (conv): Conv2d(1280, 448, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn): BatchNorm2d(448, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "        (branch3x3dbl_2): BasicConv2d(\n",
       "          (conv): Conv2d(448, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn): BatchNorm2d(384, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "        (branch3x3dbl_3a): BasicConv2d(\n",
       "          (conv): Conv2d(384, 384, kernel_size=(1, 3), stride=(1, 1), padding=(0, 1), bias=False)\n",
       "          (bn): BatchNorm2d(384, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "        (branch3x3dbl_3b): BasicConv2d(\n",
       "          (conv): Conv2d(384, 384, kernel_size=(3, 1), stride=(1, 1), padding=(1, 0), bias=False)\n",
       "          (bn): BatchNorm2d(384, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "        (branch_pool): BasicConv2d(\n",
       "          (conv): Conv2d(1280, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn): BatchNorm2d(192, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "      )\n",
       "      (17): InceptionE(\n",
       "        (branch1x1): BasicConv2d(\n",
       "          (conv): Conv2d(2048, 320, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn): BatchNorm2d(320, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "        (branch3x3_1): BasicConv2d(\n",
       "          (conv): Conv2d(2048, 384, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn): BatchNorm2d(384, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "        (branch3x3_2a): BasicConv2d(\n",
       "          (conv): Conv2d(384, 384, kernel_size=(1, 3), stride=(1, 1), padding=(0, 1), bias=False)\n",
       "          (bn): BatchNorm2d(384, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "        (branch3x3_2b): BasicConv2d(\n",
       "          (conv): Conv2d(384, 384, kernel_size=(3, 1), stride=(1, 1), padding=(1, 0), bias=False)\n",
       "          (bn): BatchNorm2d(384, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "        (branch3x3dbl_1): BasicConv2d(\n",
       "          (conv): Conv2d(2048, 448, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn): BatchNorm2d(448, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "        (branch3x3dbl_2): BasicConv2d(\n",
       "          (conv): Conv2d(448, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn): BatchNorm2d(384, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "        (branch3x3dbl_3a): BasicConv2d(\n",
       "          (conv): Conv2d(384, 384, kernel_size=(1, 3), stride=(1, 1), padding=(0, 1), bias=False)\n",
       "          (bn): BatchNorm2d(384, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "        (branch3x3dbl_3b): BasicConv2d(\n",
       "          (conv): Conv2d(384, 384, kernel_size=(3, 1), stride=(1, 1), padding=(1, 0), bias=False)\n",
       "          (bn): BatchNorm2d(384, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "        (branch_pool): BasicConv2d(\n",
       "          (conv): Conv2d(2048, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn): BatchNorm2d(192, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (avgpool): AdaptiveMaxPool2d(output_size=(1, 1))\n",
       "    (drop): Dropout(p=0.4, inplace=False)\n",
       "    (ffn): Sequential(\n",
       "      (0): Linear(in_features=2048, out_features=512, bias=True)\n",
       "      (1): ReLU()\n",
       "      (2): Linear(in_features=512, out_features=128, bias=True)\n",
       "      (3): ReLU()\n",
       "    )\n",
       "    (atten): Linear(in_features=128, out_features=1, bias=False)\n",
       "  )\n",
       "  (dense): Linear(in_features=256, out_features=256, bias=True)\n",
       "  (cl): Linear(in_features=256, out_features=2, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "\n",
    "class_weights=compute_class_weight(class_weight='balanced', classes=np.unique(dm.train_data['label']), y=dm.train_data['label'])\n",
    "class_weights=torch.tensor(class_weights,dtype=torch.float).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fri Apr 29 20:33:20 2022       \n",
      "+-----------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 465.19.01    Driver Version: 465.19.01    CUDA Version: 11.3     |\n",
      "|-------------------------------+----------------------+----------------------+\n",
      "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
      "|                               |                      |               MIG M. |\n",
      "|===============================+======================+======================|\n",
      "|   0  NVIDIA GeForce ...  Off  | 00000000:5E:00.0 Off |                  N/A |\n",
      "|  0%   50C    P2   129W / 250W |   4483MiB / 11019MiB |     30%      Default |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "|   1  NVIDIA GeForce ...  Off  | 00000000:86:00.0 Off |                  N/A |\n",
      "|  0%   24C    P8     5W / 250W |      3MiB / 11019MiB |      0%      Default |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "                                                                               \n",
      "+-----------------------------------------------------------------------------+\n",
      "| Processes:                                                                  |\n",
      "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
      "|        ID   ID                                                   Usage      |\n",
      "|=============================================================================|\n",
      "|    0   N/A  N/A   2636298      C   ...da3/envs/AI_DA/bin/python     3447MiB |\n",
      "|    0   N/A  N/A   2640101      C   ...da3/envs/AI_DA/bin/python     1033MiB |\n",
      "+-----------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 927
    },
    "id": "7mSKvEYdVLGV",
    "outputId": "304f54d4-de09-4cfb-81a8-453bf339bcdc"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "160,473,802 total parameters.\n",
      "1,640,962 training parameters.\n",
      "Epoch 0 of 20\n",
      "Epoch 0 Step 99 ave_loss 0.6339\n",
      "Epoch 0 Step 199 ave_loss 0.5528\n",
      "Epoch 0 Step 299 ave_loss 0.3966\n",
      "Epoch 0 Step 399 ave_loss 0.3862\n",
      "Epoch 0 Step 499 ave_loss 0.5016\n",
      "Epoch 0 Step 599 ave_loss 0.3857\n",
      "Epoch 0 Step 699 ave_loss 0.3917\n",
      "Epoch 0 Step 799 ave_loss 0.3542\n",
      "Epoch 0 Step 899 ave_loss 0.3671\n",
      "[[1835  583]\n",
      " [ 855 4707]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.86      0.78      0.82       303\n",
      "         1.0       0.91      0.95      0.93       695\n",
      "\n",
      "    accuracy                           0.89       998\n",
      "   macro avg       0.88      0.86      0.87       998\n",
      "weighted avg       0.89      0.89      0.89       998\n",
      "\n",
      "[[236  67]\n",
      " [ 38 657]]\n",
      "Train acc 0.7929853390141899\n",
      "Train Loss: 0.4356\n",
      "Val Loss: 0.3424\n",
      "Val Acc: 0.8720\n",
      "Better ver saved\n",
      "Saving....my_checkpoint_attention_seed_84.pth.tar\n",
      "Better ver saved\n",
      "Saving....my_checkpoint_attention_seed_84_f1.pth.tar\n",
      "Epoch 1 of 20\n",
      "Epoch 1 Step 99 ave_loss 0.3618\n",
      "Epoch 1 Step 199 ave_loss 0.3339\n",
      "Epoch 1 Step 299 ave_loss 0.3348\n",
      "Epoch 1 Step 399 ave_loss 0.3284\n",
      "Epoch 1 Step 499 ave_loss 0.3218\n",
      "Epoch 1 Step 599 ave_loss 0.3655\n",
      "Epoch 1 Step 699 ave_loss 0.3130\n",
      "Epoch 1 Step 799 ave_loss 0.2933\n",
      "Epoch 1 Step 899 ave_loss 0.2900\n",
      "[[1993  425]\n",
      " [ 486 5076]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.85      0.86      0.86       303\n",
      "         1.0       0.94      0.94      0.94       695\n",
      "\n",
      "    accuracy                           0.91       998\n",
      "   macro avg       0.90      0.90      0.90       998\n",
      "weighted avg       0.91      0.91      0.91       998\n",
      "\n",
      "[[260  43]\n",
      " [ 45 650]]\n",
      "Train acc 0.865810587370371\n",
      "Train Loss: 0.3236\n",
      "Val Loss: 0.2612\n",
      "Val Acc: 0.8959\n",
      "Better ver saved\n",
      "Saving....my_checkpoint_attention_seed_84.pth.tar\n",
      "Better ver saved\n",
      "Saving....my_checkpoint_attention_seed_84_f1.pth.tar\n",
      "Epoch 2 of 20\n",
      "Epoch 2 Step 99 ave_loss 0.2676\n",
      "Epoch 2 Step 199 ave_loss 0.2857\n",
      "Epoch 2 Step 299 ave_loss 0.2891\n",
      "Epoch 2 Step 399 ave_loss 0.2879\n",
      "Epoch 2 Step 499 ave_loss 0.2933\n",
      "Epoch 2 Step 599 ave_loss 0.3145\n",
      "Epoch 2 Step 699 ave_loss 0.2929\n",
      "Epoch 2 Step 799 ave_loss 0.2559\n",
      "Epoch 2 Step 899 ave_loss 0.2896\n",
      "[[2069  349]\n",
      " [ 389 5173]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.87      0.84      0.85       303\n",
      "         1.0       0.93      0.94      0.94       695\n",
      "\n",
      "    accuracy                           0.91       998\n",
      "   macro avg       0.90      0.89      0.90       998\n",
      "weighted avg       0.91      0.91      0.91       998\n",
      "\n",
      "[[255  48]\n",
      " [ 39 656]]\n",
      "Train acc 0.8910319851479447\n",
      "Train Loss: 0.2803\n",
      "Val Loss: 0.2692\n",
      "Val Acc: 0.8960\n",
      "Better ver saved\n",
      "Saving....my_checkpoint_attention_seed_84_f1.pth.tar\n",
      "INFO: Early stopping counter 1 of 8\n",
      "Epoch 3 of 20\n",
      "Epoch 3 Step 99 ave_loss 0.2636\n",
      "Epoch 3 Step 199 ave_loss 0.2497\n",
      "Epoch 3 Step 299 ave_loss 0.2461\n",
      "Epoch 3 Step 399 ave_loss 0.2668\n",
      "Epoch 3 Step 499 ave_loss 0.2343\n",
      "Epoch 3 Step 599 ave_loss 0.2513\n",
      "Epoch 3 Step 699 ave_loss 0.3007\n",
      "Epoch 3 Step 799 ave_loss 0.2301\n",
      "Epoch 3 Step 899 ave_loss 0.2785\n",
      "[[2076  342]\n",
      " [ 338 5224]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.86      0.89      0.87       303\n",
      "         1.0       0.95      0.94      0.94       695\n",
      "\n",
      "    accuracy                           0.92       998\n",
      "   macro avg       0.90      0.91      0.91       998\n",
      "weighted avg       0.92      0.92      0.92       998\n",
      "\n",
      "[[269  34]\n",
      " [ 44 651]]\n",
      "Train acc 0.8990822029984622\n",
      "Train Loss: 0.2583\n",
      "Val Loss: 0.2433\n",
      "Val Acc: 0.9084\n",
      "Better ver saved\n",
      "Saving....my_checkpoint_attention_seed_84.pth.tar\n",
      "Better ver saved\n",
      "Saving....my_checkpoint_attention_seed_84_f1.pth.tar\n",
      "Epoch 4 of 20\n",
      "Epoch 4 Step 99 ave_loss 0.2190\n",
      "Epoch 4 Step 199 ave_loss 0.2686\n",
      "Epoch 4 Step 299 ave_loss 0.2344\n",
      "Epoch 4 Step 399 ave_loss 0.2263\n",
      "Epoch 4 Step 499 ave_loss 0.2269\n",
      "Epoch 4 Step 599 ave_loss 0.2240\n",
      "Epoch 4 Step 699 ave_loss 0.2667\n",
      "Epoch 4 Step 799 ave_loss 0.2586\n",
      "Epoch 4 Step 899 ave_loss 0.2446\n",
      "[[2096  322]\n",
      " [ 278 5284]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.94      0.85      0.89       303\n",
      "         1.0       0.94      0.98      0.96       695\n",
      "\n",
      "    accuracy                           0.94       998\n",
      "   macro avg       0.94      0.92      0.93       998\n",
      "weighted avg       0.94      0.94      0.94       998\n",
      "\n",
      "[[259  44]\n",
      " [ 17 678]]\n",
      "Train acc 0.9105331952490062\n",
      "Train Loss: 0.2410\n",
      "Val Loss: 0.2456\n",
      "Val Acc: 0.9258\n",
      "Better ver saved\n",
      "Saving....my_checkpoint_attention_seed_84_f1.pth.tar\n",
      "INFO: Early stopping counter 1 of 8\n",
      "Epoch 5 of 20\n",
      "Epoch 5 Step 99 ave_loss 0.2348\n",
      "Epoch 5 Step 199 ave_loss 0.1803\n",
      "Epoch 5 Step 299 ave_loss 0.2016\n",
      "Epoch 5 Step 399 ave_loss 0.2488\n",
      "Epoch 5 Step 499 ave_loss 0.2083\n",
      "Epoch 5 Step 599 ave_loss 0.2246\n",
      "Epoch 5 Step 699 ave_loss 0.2823\n",
      "Epoch 5 Step 799 ave_loss 0.2434\n",
      "Epoch 5 Step 899 ave_loss 0.2662\n",
      "[[2106  312]\n",
      " [ 280 5282]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.97      0.85      0.91       303\n",
      "         1.0       0.94      0.99      0.96       695\n",
      "\n",
      "    accuracy                           0.95       998\n",
      "   macro avg       0.95      0.92      0.93       998\n",
      "weighted avg       0.95      0.95      0.94       998\n",
      "\n",
      "[[258  45]\n",
      " [  9 686]]\n",
      "Train acc 0.911851871975935\n",
      "Train Loss: 0.2333\n",
      "Val Loss: 0.2489\n",
      "Val Acc: 0.9337\n",
      "Better ver saved\n",
      "Saving....my_checkpoint_attention_seed_84_f1.pth.tar\n",
      "INFO: Early stopping counter 2 of 8\n",
      "Epoch 6 of 20\n",
      "Epoch 6 Step 99 ave_loss 0.2306\n",
      "Epoch 6 Step 199 ave_loss 0.2167\n",
      "Epoch 6 Step 299 ave_loss 0.2211\n",
      "Epoch 6 Step 399 ave_loss 0.2143\n",
      "Epoch 6 Step 499 ave_loss 0.2412\n",
      "Epoch 6 Step 599 ave_loss 0.2008\n",
      "Epoch 6 Step 699 ave_loss 0.2033\n",
      "Epoch 6 Step 799 ave_loss 0.2418\n",
      "Epoch 6 Step 899 ave_loss 0.1906\n",
      "[[2132  286]\n",
      " [ 242 5320]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.92      0.88      0.90       303\n",
      "         1.0       0.95      0.97      0.96       695\n",
      "\n",
      "    accuracy                           0.94       998\n",
      "   macro avg       0.94      0.92      0.93       998\n",
      "weighted avg       0.94      0.94      0.94       998\n",
      "\n",
      "[[266  37]\n",
      " [ 22 673]]\n",
      "Train acc 0.9212692118191255\n",
      "Train Loss: 0.2206\n",
      "Val Loss: 0.2331\n",
      "Val Acc: 0.9291\n",
      "Better ver saved\n",
      "Saving....my_checkpoint_attention_seed_84.pth.tar\n",
      "Epoch 7 of 20\n",
      "Epoch 7 Step 99 ave_loss 0.1801\n",
      "Epoch 7 Step 199 ave_loss 0.1983\n",
      "Epoch 7 Step 299 ave_loss 0.1759\n",
      "Epoch 7 Step 399 ave_loss 0.2112\n",
      "Epoch 7 Step 499 ave_loss 0.2228\n",
      "Epoch 7 Step 599 ave_loss 0.2330\n",
      "Epoch 7 Step 699 ave_loss 0.1903\n",
      "Epoch 7 Step 799 ave_loss 0.2372\n",
      "Epoch 7 Step 899 ave_loss 0.2194\n",
      "[[2146  272]\n",
      " [ 244 5318]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.93      0.86      0.90       303\n",
      "         1.0       0.94      0.97      0.96       695\n",
      "\n",
      "    accuracy                           0.94       998\n",
      "   macro avg       0.94      0.92      0.93       998\n",
      "weighted avg       0.94      0.94      0.94       998\n",
      "\n",
      "[[261  42]\n",
      " [ 19 676]]\n",
      "Epoch 00008: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Train acc 0.9232045705746281\n",
      "Train Loss: 0.2079\n",
      "Val Loss: 0.2496\n",
      "Val Acc: 0.9261\n",
      "INFO: Early stopping counter 1 of 8\n",
      "Epoch 8 of 20\n",
      "Epoch 8 Step 99 ave_loss 0.2000\n",
      "Epoch 8 Step 199 ave_loss 0.2328\n",
      "Epoch 8 Step 299 ave_loss 0.1548\n",
      "Epoch 8 Step 399 ave_loss 0.1588\n",
      "Epoch 8 Step 499 ave_loss 0.1823\n",
      "Epoch 8 Step 599 ave_loss 0.1902\n",
      "Epoch 8 Step 699 ave_loss 0.2069\n",
      "Epoch 8 Step 799 ave_loss 0.1878\n",
      "Epoch 8 Step 899 ave_loss 0.1801\n",
      "[[2164  254]\n",
      " [ 189 5373]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.91      0.88      0.89       303\n",
      "         1.0       0.95      0.96      0.95       695\n",
      "\n",
      "    accuracy                           0.94       998\n",
      "   macro avg       0.93      0.92      0.92       998\n",
      "weighted avg       0.94      0.94      0.94       998\n",
      "\n",
      "[[267  36]\n",
      " [ 27 668]]\n",
      "Train acc 0.9337774458434761\n",
      "Train Loss: 0.1897\n",
      "Val Loss: 0.2358\n",
      "Val Acc: 0.9247\n",
      "INFO: Early stopping counter 2 of 8\n",
      "Epoch 9 of 20\n",
      "Epoch 9 Step 99 ave_loss 0.1951\n",
      "Epoch 9 Step 199 ave_loss 0.1520\n",
      "Epoch 9 Step 299 ave_loss 0.1691\n",
      "Epoch 9 Step 399 ave_loss 0.1771\n",
      "Epoch 9 Step 499 ave_loss 0.1689\n",
      "Epoch 9 Step 599 ave_loss 0.1584\n",
      "Epoch 9 Step 699 ave_loss 0.1755\n",
      "Epoch 9 Step 799 ave_loss 0.1905\n",
      "Epoch 9 Step 899 ave_loss 0.1749\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[2178  240]\n",
      " [ 169 5393]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.92      0.87      0.90       303\n",
      "         1.0       0.95      0.97      0.96       695\n",
      "\n",
      "    accuracy                           0.94       998\n",
      "   macro avg       0.93      0.92      0.93       998\n",
      "weighted avg       0.94      0.94      0.94       998\n",
      "\n",
      "[[264  39]\n",
      " [ 22 673]]\n",
      "Epoch 00010: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Train acc 0.9388158125980952\n",
      "Train Loss: 0.1768\n",
      "Val Loss: 0.2365\n",
      "Val Acc: 0.9265\n",
      "INFO: Early stopping counter 3 of 8\n",
      "Epoch 10 of 20\n",
      "Epoch 10 Step 99 ave_loss 0.2111\n",
      "Epoch 10 Step 199 ave_loss 0.1590\n",
      "Epoch 10 Step 299 ave_loss 0.1936\n",
      "Epoch 10 Step 399 ave_loss 0.1634\n",
      "Epoch 10 Step 499 ave_loss 0.1703\n",
      "Epoch 10 Step 599 ave_loss 0.1698\n",
      "Epoch 10 Step 699 ave_loss 0.1762\n",
      "Epoch 10 Step 799 ave_loss 0.1544\n",
      "Epoch 10 Step 899 ave_loss 0.1523\n",
      "[[2187  231]\n",
      " [ 168 5394]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.93      0.87      0.90       303\n",
      "         1.0       0.95      0.97      0.96       695\n",
      "\n",
      "    accuracy                           0.94       998\n",
      "   macro avg       0.94      0.92      0.93       998\n",
      "weighted avg       0.94      0.94      0.94       998\n",
      "\n",
      "[[264  39]\n",
      " [ 21 674]]\n",
      "Train acc 0.9403691891858181\n",
      "Train Loss: 0.1730\n",
      "Val Loss: 0.2395\n",
      "Val Acc: 0.9277\n",
      "INFO: Early stopping counter 4 of 8\n",
      "Epoch 11 of 20\n",
      "Epoch 11 Step 99 ave_loss 0.1680\n",
      "Epoch 11 Step 199 ave_loss 0.1772\n",
      "Epoch 11 Step 299 ave_loss 0.2313\n",
      "Epoch 11 Step 399 ave_loss 0.1769\n",
      "Epoch 11 Step 499 ave_loss 0.1466\n",
      "Epoch 11 Step 599 ave_loss 0.1636\n",
      "Epoch 11 Step 699 ave_loss 0.1425\n",
      "Epoch 11 Step 799 ave_loss 0.1524\n",
      "Epoch 11 Step 899 ave_loss 0.1516\n",
      "[[2188  230]\n",
      " [ 160 5402]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.93      0.86      0.90       303\n",
      "         1.0       0.94      0.97      0.96       695\n",
      "\n",
      "    accuracy                           0.94       998\n",
      "   macro avg       0.94      0.92      0.93       998\n",
      "weighted avg       0.94      0.94      0.94       998\n",
      "\n",
      "[[262  41]\n",
      " [ 20 675]]\n",
      "Epoch 00012: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Train acc 0.9416651402859468\n",
      "Train Loss: 0.1720\n",
      "Val Loss: 0.2421\n",
      "Val Acc: 0.9262\n",
      "INFO: Early stopping counter 5 of 8\n",
      "Epoch 12 of 20\n",
      "Epoch 12 Step 99 ave_loss 0.1748\n",
      "Epoch 12 Step 199 ave_loss 0.1669\n",
      "Epoch 12 Step 299 ave_loss 0.1420\n",
      "Epoch 12 Step 399 ave_loss 0.1854\n",
      "Epoch 12 Step 499 ave_loss 0.1645\n",
      "Epoch 12 Step 599 ave_loss 0.1847\n",
      "Epoch 12 Step 699 ave_loss 0.1410\n",
      "Epoch 12 Step 799 ave_loss 0.1853\n",
      "Epoch 12 Step 899 ave_loss 0.1803\n",
      "[[2190  228]\n",
      " [ 149 5413]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.93      0.86      0.90       303\n",
      "         1.0       0.94      0.97      0.96       695\n",
      "\n",
      "    accuracy                           0.94       998\n",
      "   macro avg       0.94      0.92      0.93       998\n",
      "weighted avg       0.94      0.94      0.94       998\n",
      "\n",
      "[[262  41]\n",
      " [ 20 675]]\n",
      "Train acc 0.9435483351922818\n",
      "Train Loss: 0.1697\n",
      "Val Loss: 0.2421\n",
      "Val Acc: 0.9262\n",
      "INFO: Early stopping counter 6 of 8\n",
      "Epoch 13 of 20\n",
      "Epoch 13 Step 99 ave_loss 0.1807\n",
      "Epoch 13 Step 199 ave_loss 0.1431\n",
      "Epoch 13 Step 299 ave_loss 0.1862\n",
      "Epoch 13 Step 399 ave_loss 0.1539\n",
      "Epoch 13 Step 499 ave_loss 0.1764\n",
      "Epoch 13 Step 599 ave_loss 0.1835\n",
      "Epoch 13 Step 699 ave_loss 0.1813\n",
      "Epoch 13 Step 799 ave_loss 0.1515\n",
      "Epoch 13 Step 899 ave_loss 0.1868\n",
      "[[2182  236]\n",
      " [ 148 5414]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.93      0.87      0.90       303\n",
      "         1.0       0.94      0.97      0.96       695\n",
      "\n",
      "    accuracy                           0.94       998\n",
      "   macro avg       0.94      0.92      0.93       998\n",
      "weighted avg       0.94      0.94      0.94       998\n",
      "\n",
      "[[263  40]\n",
      " [ 21 674]]\n",
      "Train acc 0.9424374113546052\n",
      "Train Loss: 0.1706\n",
      "Val Loss: 0.2419\n",
      "Val Acc: 0.9264\n",
      "INFO: Early stopping counter 7 of 8\n",
      "Epoch 14 of 20\n",
      "Epoch 14 Step 99 ave_loss 0.1636\n",
      "Epoch 14 Step 199 ave_loss 0.1663\n",
      "Epoch 14 Step 299 ave_loss 0.1514\n",
      "Epoch 14 Step 399 ave_loss 0.1861\n",
      "Epoch 14 Step 499 ave_loss 0.1692\n",
      "Epoch 14 Step 599 ave_loss 0.1517\n",
      "Epoch 14 Step 699 ave_loss 0.1727\n",
      "Epoch 14 Step 799 ave_loss 0.1482\n",
      "Epoch 14 Step 899 ave_loss 0.2154\n",
      "[[2189  229]\n",
      " [ 160 5402]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.93      0.87      0.90       303\n",
      "         1.0       0.94      0.97      0.96       695\n",
      "\n",
      "    accuracy                           0.94       998\n",
      "   macro avg       0.94      0.92      0.93       998\n",
      "weighted avg       0.94      0.94      0.94       998\n",
      "\n",
      "[[263  40]\n",
      " [ 21 674]]\n",
      "Train acc 0.9418217254254646\n",
      "Train Loss: 0.1715\n",
      "Val Loss: 0.2418\n",
      "Val Acc: 0.9264\n",
      "INFO: Early stopping counter 8 of 8\n",
      "INFO: Early stopping\n",
      "test               precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.93      0.89      0.91       302\n",
      "         1.0       0.95      0.97      0.96       695\n",
      "\n",
      "    accuracy                           0.95       997\n",
      "   macro avg       0.94      0.93      0.94       997\n",
      "weighted avg       0.95      0.95      0.95       997\n",
      "\n",
      "test_ac 0.9478435305917753\n",
      "Confusion [[269  33]\n",
      " [ 19 676]]\n",
      "Macro_f1 0.937413684871312\n"
     ]
    }
   ],
   "source": [
    "from torch.optim import lr_scheduler\n",
    "\n",
    "train_loss_list=[]\n",
    "epoch_list=[]\n",
    "val_loss_list=[]\n",
    "val_f1_list=[]\n",
    "train_f1_list=[]\n",
    "\n",
    "\n",
    "#define model\n",
    "model.to(device)\n",
    "total_params = sum(p.numel() for p in model.parameters())\n",
    "print(f\"{total_params:,} total parameters.\")\n",
    "total_trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "print(f\"{total_trainable_params:,} training parameters.\")\n",
    "# define hyper\n",
    "# grouped_params = [\n",
    "#     {\"params\": [p for n, p in roberta_params], \"lr\": 1e-5},\n",
    "#     # {\"params\": [p for n, p in classifier_params], \"lr\": 3e-3}\n",
    "# ]\n",
    "# optimizer = torch.optim.Adam([\n",
    "#         {'params': model.BERT.lstm.parameters()},  \n",
    "#         {'params': model.incept.flat.parameters()}, \n",
    "#         {'params': model.incept.avgpool.parameters()},  \n",
    "#         {'params': model.dense.parameters()},       \n",
    "#         {'params': model.cl.parameters()},       \n",
    "#         {'params': model.BERT.PhoBERT.parameters(), 'lr': 1e-5},\n",
    "#         {'params': model.incept.incept.parameters(), 'lr': 1e-5}\n",
    "#     ], lr=1e-3)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)\n",
    "#training\n",
    "# def training(model, optimizer):\n",
    "# define hyper\n",
    "criterion = nn.CrossEntropyLoss(weight=class_weights)\n",
    "# optimizer = torch.optim.SGD(model.parameters(), lr=0.001, momentum=0.9, weight_decay=5e-4)\n",
    "epochs = 20\n",
    "lr_scheduler = LRScheduler(optimizer)\n",
    "early_stopping = EarlyStopping()\n",
    "min_loss = np.Inf\n",
    "max_f1 = 0\n",
    "\n",
    "for epoch in range(epochs):\n",
    "  print(f\"Epoch {epoch} of {epochs}\")\n",
    "  train_epoch_loss, train_ac = fit(\n",
    "        model, dm, criterion, optimizer, epoch\n",
    "    )\n",
    "  val_epoch_loss, val_epoch_ac = validation(\n",
    "        model, dm, criterion\n",
    "    )\n",
    "\n",
    "  lr_scheduler(val_epoch_ac)\n",
    "\n",
    "  print('Train acc', train_ac)\n",
    "  print(f\"Train Loss: {train_epoch_loss:.4f}\")\n",
    "  print(f'Val Loss: {val_epoch_loss:.4f}')\n",
    "  print(f'Val Acc: {val_epoch_ac:.4f}')\n",
    "    \n",
    "  train_loss_list.append(train_epoch_loss)\n",
    "  train_f1_list.append(train_ac)\n",
    "  val_loss_list.append(val_epoch_loss)\n",
    "  val_f1_list.append(val_epoch_ac)\n",
    "  epoch_list.append(epoch)\n",
    "    \n",
    "  checkpoint ={'state_dict': model.state_dict(), 'optimizer': optimizer.state_dict(), 'epoch': epoch, 'loss':val_epoch_loss }\n",
    "  if (min_loss >= val_epoch_loss):\n",
    "      print('Better ver saved')\n",
    "      min_loss = val_epoch_loss\n",
    "      save_checkpoint(checkpoint)\n",
    "        \n",
    "  if (val_epoch_ac >= max_f1):\n",
    "      checkpoint ={'state_dict': model.state_dict(), 'optimizer': optimizer.state_dict(), 'epoch': epoch, 'loss': val_epoch_loss, \"f1\": val_epoch_ac}\n",
    "      print('Better ver saved')\n",
    "      max_f1 = val_epoch_ac\n",
    "      save_checkpoint1(checkpoint)\n",
    "    \n",
    "  early_stopping(val_epoch_loss)\n",
    "  if early_stopping.early_stop:\n",
    "      break\n",
    "\n",
    "testing(model, dm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_graph(train_accu, eval_accu, epoch_list, type='F1'):\n",
    "    plt.plot(epoch_list,train_accu ,'-o')\n",
    "    plt.plot(epoch_list,eval_accu,'-o')\n",
    "    plt.xlabel('epoch')\n",
    "    plt.ylabel(type)\n",
    "    plt.legend(['Train','Valid'])\n",
    "    plt.title('Train vs Valid '+type)\n",
    "\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# freeze inception seed 66 \n",
    "show_graph(train_f1_list, val_f1_list, epoch_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# freeze inception seed 84 \n",
    "show_graph(train_f1_list, val_f1_list, epoch_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# freeze inception seed 84 \n",
    "show_graph(train_loss_list, val_loss_list, epoch_list, 'loss')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# freeze inception seed 66 \n",
    "show_graph(train_loss_list, val_loss_list, epoch_list, 'loss')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6992"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import gc\n",
    "del model \n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "KZ1CVGaDdnXX",
    "outputId": "2be27769-a799-40e1-9bf7-a3524561c637"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at vinai/phobert-base were not used when initializing RobertaModel: ['lm_head.dense.weight', 'lm_head.layer_norm.weight', 'lm_head.dense.bias', 'lm_head.bias', 'lm_head.decoder.bias', 'lm_head.layer_norm.bias', 'lm_head.decoder.weight']\n",
      "- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6\n"
     ]
    }
   ],
   "source": [
    "PATH = \"my_checkpoint_attention_seed_84.pth.tar\"\n",
    "checkpoint = torch.load(PATH, map_location=device)\n",
    "model = Concatmodal()\n",
    "model.load_state_dict(checkpoint['state_dict'])\n",
    "model.to(device)\n",
    "print(checkpoint['epoch'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test               precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.93      0.89      0.91       302\n",
      "         1.0       0.95      0.97      0.96       695\n",
      "\n",
      "    accuracy                           0.95       997\n",
      "   macro avg       0.94      0.93      0.94       997\n",
      "weighted avg       0.95      0.95      0.95       997\n",
      "\n",
      "test_ac 0.9478435305917753\n",
      "Confusion [[269  33]\n",
      " [ 19 676]]\n",
      "Macro_f1 0.937413684871312\n"
     ]
    }
   ],
   "source": [
    "#f1\n",
    "testing(model, dm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test               precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.94      0.91      0.92       302\n",
      "         1.0       0.96      0.97      0.97       695\n",
      "\n",
      "    accuracy                           0.95       997\n",
      "   macro avg       0.95      0.94      0.94       997\n",
      "weighted avg       0.95      0.95      0.95       997\n",
      "\n",
      "test_ac 0.9538615847542627\n",
      "Confusion [[274  28]\n",
      " [ 18 677]]\n",
      "Macro_f1 0.9448508898508898\n"
     ]
    }
   ],
   "source": [
    "#valloss \n",
    "testing(model, dm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "id": "oegFhN6sJeRV"
   },
   "outputs": [],
   "source": [
    "optimizer = torch.optim.Adam([\n",
    "        {'params': model.BERT.lstm.parameters()},  \n",
    "        {'params': model.incept.flat.parameters()}, \n",
    "        {'params': model.incept.avgpool.parameters()},  \n",
    "        {'params': model.dense.parameters()},       \n",
    "        {'params': model.cl.parameters()},       \n",
    "        {'params': model.BERT.PhoBERT.parameters(), 'lr': 1e-5},\n",
    "        {'params': model.incept.incept.parameters(), 'lr': 1e-5}\n",
    "    ], lr=1e-3)\n",
    "optimizer.load_state_dict(checkpoint['optimizer'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "R1VyLADEpnFs"
   },
   "outputs": [],
   "source": [
    "import cv2\n",
    "def plot(model):\n",
    "  model.eval()\n",
    "  test_preds = torch.tensor([], device=device)\n",
    "  test_labels = torch.tensor([], device = device)\n",
    "  test_text = []\n",
    "  test_img = []\n",
    "\n",
    "  soft_m = nn.Softmax(dim=-1)\n",
    "  with torch.no_grad():\n",
    "    for i, (texts, images, labels) in enumerate(dm.test_dataloader()):\n",
    "        labels = labels.to(device)\n",
    "\n",
    "        outputs = model(images, texts)\n",
    "\n",
    "        output_scores = soft_m(outputs)\n",
    "        predictions = torch.argmax(output_scores, dim=-1)\n",
    "\n",
    "        test_preds = torch.cat((test_preds, predictions), dim=0)\n",
    "        test_labels = torch.cat((test_labels, labels), dim=0)\n",
    "        for t in texts:\n",
    "          test_text.append(t) \n",
    "        for i in images:\n",
    "          test_img.append(i) \n",
    "\n",
    "    \n",
    "    test_preds = test_preds.cpu().numpy()\n",
    "    test_labels = test_labels.cpu().numpy()\n",
    "  i = 1\n",
    "  plt.figure(figsize=(40,40))\n",
    "  print(len(test_labels))\n",
    "  for image, actual_label, label, text in zip(test_img, test_labels, test_preds, test_text):\n",
    "\n",
    "      if (actual_label == label):\n",
    "\n",
    "        plt.subplot(4,4,i)\n",
    "        i+=1\n",
    "        # image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "        # image = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
    "        image = T.ToPILImage()(image).convert(\"RGB\")\n",
    "        plt.imshow(image)\n",
    "        plt.axis('off')\n",
    "        plt.title(\"Text: {}...\\n Actual: {}\\n Predicted: {}\".format(text[:30], actual_label, label))\n",
    "        # plt.xlabel(text)\n",
    "      else:\n",
    "        pass\n",
    "      if (i==17):\n",
    "        break;\n",
    "  plt.savefig('res_freeze_af20epoch.png', bbox_inches='tight')\n",
    "  plt.show()\n",
    "  \n",
    "\n",
    "plot(model)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sun Mar 20 23:13:56 2022       \r\n",
      "+-----------------------------------------------------------------------------+\r\n",
      "| NVIDIA-SMI 465.19.01    Driver Version: 465.19.01    CUDA Version: 11.3     |\r\n",
      "|-------------------------------+----------------------+----------------------+\r\n",
      "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\r\n",
      "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\r\n",
      "|                               |                      |               MIG M. |\r\n",
      "|===============================+======================+======================|\r\n",
      "|   0  NVIDIA GeForce ...  Off  | 00000000:5E:00.0 Off |                  N/A |\r\n",
      "|  0%   35C    P2    43W / 250W |  11004MiB / 11019MiB |      0%      Default |\r\n",
      "|                               |                      |                  N/A |\r\n",
      "+-------------------------------+----------------------+----------------------+\r\n",
      "|   1  NVIDIA GeForce ...  Off  | 00000000:86:00.0 Off |                  N/A |\r\n",
      "|  0%   24C    P8     3W / 250W |   3496MiB / 11019MiB |      0%      Default |\r\n",
      "|                               |                      |                  N/A |\r\n",
      "+-------------------------------+----------------------+----------------------+\r\n",
      "                                                                               \r\n",
      "+-----------------------------------------------------------------------------+\r\n",
      "| Processes:                                                                  |\r\n",
      "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\r\n",
      "|        ID   ID                                                   Usage      |\r\n",
      "|=============================================================================|\r\n",
      "|    0   N/A  N/A   3422639      C   ...da3/envs/AI_DA/bin/python    11001MiB |\r\n",
      "|    1   N/A  N/A    429695      C   python                           3493MiB |\r\n",
      "+-----------------------------------------------------------------------------+\r\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hBUVnSf-_ht0"
   },
   "source": [
    "# Multimodal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "8dlaSCOAxb0i"
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_auc_score, classification_report, accuracy_score\n",
    "\n",
    "class Multimodal(pl.LightningModule):\n",
    "  def __init__(self, lr_multi, lr_classifier):\n",
    "    super(Multimodal,self).__init__()\n",
    "    self.loss_module = nn.CrossEntropyLoss()\n",
    "    self.BERT = Bert_Lstm()\n",
    "    self.incept = Inception()\n",
    "    self.dense = nn.Linear(256,256)\n",
    "    self.cl = nn.Linear(256,3)\n",
    "    self.bpe_path = \"/content/drive/MyDrive/NLP/PhoBERT_base_transformers/bpe.codes\"\n",
    "    self.vocab_path = \"/content/drive/MyDrive/NLP/PhoBERT_base_transformers/dict.txt\"\n",
    "    self.lr_multi = lr_multi\n",
    "    self.lr_classifier = lr_classifier\n",
    "\n",
    "    # self.model = self._build_model()\n",
    "    # self.trainer_params = self._get_trainer_params()\n",
    "\n",
    "  def forward(self, image, texts, labels=None):\n",
    "    inputs = tokenize_data(texts, self.vocab_path, self.bpe_path)\n",
    "    # b_labels = None\n",
    "    # b_labels = labels.to(self.device)\n",
    "\n",
    "    for key in inputs:\n",
    "        inputs[key] = inputs[key].to(self.device)\n",
    "    image = image.to(self.device)\n",
    "\n",
    "    fea1 = self.BERT(inputs)\n",
    "    fea2 = self.incept(image)\n",
    "    cat = torch.cat((fea1, fea2), 1)\n",
    "    out = self.cl(cat)\n",
    "\n",
    "    return out\n",
    "  \n",
    "  def configure_optimizers(self):\n",
    "    roberta_params = self.BERT.parameters()\n",
    "    inception_params = self.incept.parameters()\n",
    "    dense_params = self.dense.parameters()\n",
    "    classifier_params = self.cl.parameters()\n",
    "\n",
    "    # for n,p in classifier_params:\n",
    "    #   print(n,p)\n",
    "\n",
    "    # grouped_params = [\n",
    "    #     {\"params\": roberta_params, \"lr\": self.lr_multi},\n",
    "    #     {\"params\": inception_params, \"lr\": self.lr_multi},\n",
    "    #     {\"params\": dense_params, \"lr\": self.lr_multi},\n",
    "    #     {\"params\": classifier_params, \"lr\": self.lr_classifier}\n",
    "    # ]\n",
    "    # optimizer = torch.optim.AdamW(\n",
    "    #     grouped_params\n",
    "    # )\n",
    "    optimizer = torch.optim.AdamW(\n",
    "              self.parameters(), \n",
    "              lr = 0.001\n",
    "          )\n",
    "\n",
    "    scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=1000, gamma=0.98)\n",
    "  \n",
    "\n",
    "\n",
    "  def training_step(self, batch, batch_idx):\n",
    "    texts, images, labels = batch\n",
    "    preds = self(images, texts, labels)\n",
    "    labels = labels.to(self.device)\n",
    "\n",
    "    loss = self.loss_module(preds, labels)\n",
    "\n",
    "    return loss\n",
    "  \n",
    "  def validation_step(self, batch, batch_idx):\n",
    "    texts, images, labels = batch\n",
    "    logits = self(images, texts, labels)\n",
    "\n",
    "    loss = self.loss_module(logits, labels)\n",
    "    output_scores = torch.softmax(logits, dim=-1)\n",
    "\n",
    "    return loss, output_scores, labels\n",
    "\n",
    "  def validation_epoch_end(self, validation_step_outputs):\n",
    "    val_preds = torch.tensor([], device=self.device)\n",
    "    val_scores = torch.tensor([], device=self.device)\n",
    "    val_labels = torch.tensor([], device=self.device)\n",
    "    val_loss = 0\n",
    "    total_item = 0\n",
    "\n",
    "    for idx, item in enumerate(validation_step_outputs):\n",
    "        loss, output_scores, labels = item\n",
    "\n",
    "        predictions = torch.argmax(output_scores, dim=-1)\n",
    "        val_preds = torch.cat((val_preds, predictions), dim=0)\n",
    "        val_scores = torch.cat((val_scores, output_scores[:, 1]), dim=0)\n",
    "        val_labels = torch.cat((val_labels, labels), dim=0)\n",
    "\n",
    "        val_loss += loss\n",
    "        total_item += 1\n",
    "\n",
    "    # print(\"VAL PREDS\", val_preds.shape)\n",
    "    # print(\"VAL SCORES\", val_scores.shape)\n",
    "    # print(\"VAL LABELS\", val_labels.shape)\n",
    "    val_preds = val_preds.cpu().numpy()\n",
    "    val_scores = val_scores.cpu().numpy()\n",
    "    val_labels = val_labels.cpu().numpy()\n",
    "\n",
    "    # reports = classification_report(val_labels, val_preds, output_dict=True)\n",
    "    print(\"VAL PREDS\", val_preds)\n",
    "    print(\"VAL LABELS\", val_labels)\n",
    "    print(\"VAL SCORES\", val_scores)\n",
    "    # try:\n",
    "    #     auc = roc_auc_score(val_labels, val_scores)\n",
    "    # except Exception as e:\n",
    "    #     print(e)\n",
    "    #     print(\"Cannot calculate AUC. Default to 0\")\n",
    "    #     auc = 0\n",
    "    accuracy = accuracy_score(val_labels, val_preds)\n",
    "\n",
    "    print(classification_report(val_labels, val_preds))\n",
    "\n",
    "    self.log(\"loss/val\", val_loss)\n",
    "    # self.log(\"auc/val\", auc)\n",
    "    self.log(\"accuracy/val\", accuracy)\n",
    "    # self.log(\"precision/val\", reports[\"weighted avg\"][\"precision\"])\n",
    "    # self.log(\"recall/val\", reports[\"weighted avg\"][\"recall\"])\n",
    "    # self.log(\"f1/val\", reports[\"weighted avg\"][\"f1-score\"])\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "E1RPu7-B7wWu"
   },
   "outputs": [],
   "source": [
    "# trainer = pl.Trainer(\n",
    "#     fast_dev_run=True,\n",
    "# )\n",
    "# multi = Multimodal(lr_multi=1e-5, lr_classifier=3e-3)\n",
    "# dm = SentimentDataModule(data_df)\n",
    "# trainer.fit(multi, dm)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000,
     "referenced_widgets": [
      "a93f05a257d5421eb2c72efcdac8fcd3",
      "16559060dd074447bf5e27134b4870ae",
      "8d8bf547e78f461da79bb4e3e8449335",
      "2d90f769f9a748899fd536bc215e0902",
      "83e93a1ba3f24f96ac5ea9107e160a9e",
      "664ccebe5b894806a2c84d633e107f38",
      "81d26020a9784869b593de58ca922404",
      "2132d76e642e4cbaaabf71e032d0fb8b",
      "1f0e5ebe496b45ec97082f2bb666a82e",
      "02eca356512546b1954a563bc988d6b5",
      "7939b8ff88a940028fb77f2bd715db87",
      "9aad7677f25b4c85abc0e6edb7b9212d",
      "fff7e36afcae4a3a81e5b40705c21519",
      "e47ffdaa9228458182c11c9ba945e33d",
      "c4c4bae04bad46dea96a97dc5a7d8d0d",
      "460e2a8033f04d4fbd9109254d2dfb73",
      "f7947613432143f98f2f714598d25dfd",
      "bb16f7ad93f34d12babc320869e1413b",
      "ba20e1bc66e94c829e2a46dd5d6a3483",
      "31d14a5ec28640d8acf64959fb4a5aa8",
      "b860941628094b5c8720672f5ebb3f3d",
      "93b193607ebd4e758cfc04a19b9dd2d6"
     ]
    },
    "id": "f9_mXDl3znZO",
    "outputId": "0605ceaf-c20f-4bfd-e5cc-d55755bf15e0"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using 16bit native Automatic Mixed Precision (AMP)\n",
      "GPU available: True, used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "/usr/local/lib/python3.7/dist-packages/pytorch_lightning/core/datamodule.py:470: LightningDeprecationWarning: DataModule.setup has already been called, so it will not be called again. In v1.6 this behavior will change to always call DataModule.setup.\n",
      "  f\"DataModule.{name} has already been called, so it will not be called again. \"\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "/usr/local/lib/python3.7/dist-packages/pytorch_lightning/trainer/optimizers.py:39: UserWarning: `LightningModule.configure_optimizers` returned `None`, this fit will run with no optimizer\n",
      "  UserWarning,\n",
      "Missing logger folder: /content/drive/MyDrive/Colab Notebooks/Multimodal/tb_logs/default\n",
      "\n",
      "  | Name        | Type             | Params\n",
      "-------------------------------------------------\n",
      "0 | loss_module | CrossEntropyLoss | 0     \n",
      "1 | BERT        | Bert_Lstm        | 135 M \n",
      "2 | incept      | Inception        | 24.1 M\n",
      "3 | dense       | Linear           | 65.8 K\n",
      "4 | cl          | Linear           | 771   \n",
      "-------------------------------------------------\n",
      "159 M     Trainable params\n",
      "0         Non-trainable params\n",
      "159 M     Total params\n",
      "319.243   Total estimated model params size (MB)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a93f05a257d5421eb2c72efcdac8fcd3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation sanity check: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "VAL PREDS [2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 1. 2. 2. 2. 2. 2. 2. 0. 2. 2. 2. 2. 2. 2.\n",
      " 2. 2. 2. 1. 2. 2. 2. 2.]\n",
      "VAL LABELS [1. 2. 0. 0. 2. 2. 2. 2. 0. 2. 1. 0. 1. 1. 1. 1. 0. 2. 2. 1. 2. 1. 1. 2.\n",
      " 1. 2. 2. 2. 1. 2. 0. 1.]\n",
      "VAL SCORES [0.2503872  0.25511816 0.33502465 0.233994   0.2737939  0.30235466\n",
      " 0.30597535 0.28478307 0.34449401 0.28433585 0.3386114  0.24622746\n",
      " 0.2953767  0.28849763 0.32898384 0.27177638 0.2833459  0.30263612\n",
      " 0.31047884 0.24584286 0.32877764 0.25412712 0.20685947 0.29209316\n",
      " 0.30371505 0.292875   0.31025544 0.36505568 0.30896547 0.29549003\n",
      " 0.30231565 0.30986333]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.00      0.00      0.00         6\n",
      "         1.0       0.50      0.08      0.14        12\n",
      "         2.0       0.41      0.86      0.56        14\n",
      "\n",
      "    accuracy                           0.41        32\n",
      "   macro avg       0.30      0.31      0.23        32\n",
      "weighted avg       0.37      0.41      0.30        32\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9aad7677f25b4c85abc0e6edb7b9212d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "AssertionError",
     "evalue": "ignored",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-48-a151cafc1a4d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     26\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m \u001b[0mdm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msetup\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstage\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"fit\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 28\u001b[0;31m \u001b[0mtrainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmulti\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdm\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pytorch_lightning/trainer/trainer.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, model, train_dataloaders, val_dataloaders, datamodule, train_dataloader, ckpt_path)\u001b[0m\n\u001b[1;32m    737\u001b[0m             \u001b[0mtrain_dataloaders\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_dataloader\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    738\u001b[0m         self._call_and_handle_interrupt(\n\u001b[0;32m--> 739\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fit_impl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_dataloaders\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_dataloaders\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdatamodule\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mckpt_path\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    740\u001b[0m         )\n\u001b[1;32m    741\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pytorch_lightning/trainer/trainer.py\u001b[0m in \u001b[0;36m_call_and_handle_interrupt\u001b[0;34m(self, trainer_fn, *args, **kwargs)\u001b[0m\n\u001b[1;32m    681\u001b[0m         \"\"\"\n\u001b[1;32m    682\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 683\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mtrainer_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    684\u001b[0m         \u001b[0;31m# TODO: treat KeyboardInterrupt as BaseException (delete the code below) in v1.7\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    685\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mKeyboardInterrupt\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mexception\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pytorch_lightning/trainer/trainer.py\u001b[0m in \u001b[0;36m_fit_impl\u001b[0;34m(self, model, train_dataloaders, val_dataloaders, datamodule, ckpt_path)\u001b[0m\n\u001b[1;32m    771\u001b[0m         \u001b[0;31m# TODO: ckpt_path only in v1.7\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    772\u001b[0m         \u001b[0mckpt_path\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mckpt_path\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresume_from_checkpoint\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 773\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_run\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mckpt_path\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mckpt_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    774\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    775\u001b[0m         \u001b[0;32massert\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstopped\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pytorch_lightning/trainer/trainer.py\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, model, ckpt_path)\u001b[0m\n\u001b[1;32m   1193\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1194\u001b[0m         \u001b[0;31m# dispatch `start_training` or `start_evaluating` or `start_predicting`\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1195\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dispatch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1196\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1197\u001b[0m         \u001b[0;31m# plugin will finalized fitting (e.g. ddp_spawn will load trained model)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pytorch_lightning/trainer/trainer.py\u001b[0m in \u001b[0;36m_dispatch\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1273\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtraining_type_plugin\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstart_predicting\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1274\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1275\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtraining_type_plugin\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstart_training\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1276\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1277\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mrun_stage\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pytorch_lightning/plugins/training_type/training_type_plugin.py\u001b[0m in \u001b[0;36mstart_training\u001b[0;34m(self, trainer)\u001b[0m\n\u001b[1;32m    200\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mstart_training\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrainer\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m\"pl.Trainer\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    201\u001b[0m         \u001b[0;31m# double dispatch to initiate the training loop\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 202\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_results\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun_stage\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    203\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    204\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mstart_evaluating\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrainer\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m\"pl.Trainer\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pytorch_lightning/trainer/trainer.py\u001b[0m in \u001b[0;36mrun_stage\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1283\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredicting\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1284\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_run_predict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1285\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_run_train\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1286\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1287\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_pre_training_routine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pytorch_lightning/trainer/trainer.py\u001b[0m in \u001b[0;36m_run_train\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1313\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit_loop\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrainer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1314\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_detect_anomaly\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_detect_anomaly\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1315\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit_loop\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1316\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1317\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_run_evaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0m_EVALUATE_OUTPUT\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pytorch_lightning/loops/base.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    143\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    144\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_advance_start\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 145\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madvance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    146\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_advance_end\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    147\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrestarting\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pytorch_lightning/loops/fit_loop.py\u001b[0m in \u001b[0;36madvance\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    232\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    233\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprofiler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprofile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"run_training_epoch\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 234\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mepoch_loop\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata_fetcher\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    235\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    236\u001b[0m             \u001b[0;31m# the global step is manually decreased here due to backwards compatibility with existing loggers\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pytorch_lightning/loops/base.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    143\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    144\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_advance_start\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 145\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madvance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    146\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_advance_end\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    147\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrestarting\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pytorch_lightning/loops/epoch/training_epoch_loop.py\u001b[0m in \u001b[0;36madvance\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    191\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    192\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprofiler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprofile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"run_training_batch\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 193\u001b[0;31m                 \u001b[0mbatch_output\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbatch_loop\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_idx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    194\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    195\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbatch_progress\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mincrement_processed\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pytorch_lightning/loops/base.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    143\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    144\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_advance_start\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 145\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madvance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    146\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_advance_end\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    147\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrestarting\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pytorch_lightning/loops/batch/training_batch_loop.py\u001b[0m in \u001b[0;36madvance\u001b[0;34m(self, batch, batch_idx)\u001b[0m\n\u001b[1;32m     86\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlightning_module\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautomatic_optimization\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     87\u001b[0m             \u001b[0moptimizers\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_get_active_optimizers\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptimizers\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptimizer_frequencies\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_idx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 88\u001b[0;31m             \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptimizer_loop\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msplit_batch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizers\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_idx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     89\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     90\u001b[0m             \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmanual_loop\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msplit_batch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_idx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pytorch_lightning/loops/base.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    143\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    144\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_advance_start\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 145\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madvance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    146\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_advance_end\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    147\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrestarting\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pytorch_lightning/loops/optimization/optimizer_loop.py\u001b[0m in \u001b[0;36madvance\u001b[0;34m(self, batch, *args, **kwargs)\u001b[0m\n\u001b[1;32m    217\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_batch_idx\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    218\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_optimizers\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptim_progress\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptimizer_position\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 219\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptimizer_idx\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    220\u001b[0m         )\n\u001b[1;32m    221\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloss\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pytorch_lightning/loops/optimization/optimizer_loop.py\u001b[0m in \u001b[0;36m_run_optimization\u001b[0;34m(self, split_batch, batch_idx, optimizer, opt_idx)\u001b[0m\n\u001b[1;32m    264\u001b[0m         \u001b[0;31m# gradient update with accumulated gradients\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    265\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 266\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_optimizer_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mopt_idx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_idx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mclosure\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    267\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    268\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mclosure\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconsume_result\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pytorch_lightning/loops/optimization/optimizer_loop.py\u001b[0m in \u001b[0;36m_optimizer_step\u001b[0;34m(self, optimizer, opt_idx, batch_idx, train_step_and_backward_closure)\u001b[0m\n\u001b[1;32m    384\u001b[0m             \u001b[0mon_tpu\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_device_type\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mDeviceType\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTPU\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0m_TPU_AVAILABLE\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    385\u001b[0m             \u001b[0musing_native_amp\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mamp_backend\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mamp_backend\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mAMPType\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mNATIVE\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 386\u001b[0;31m             \u001b[0musing_lbfgs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mis_lbfgs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    387\u001b[0m         )\n\u001b[1;32m    388\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pytorch_lightning/core/lightning.py\u001b[0m in \u001b[0;36moptimizer_step\u001b[0;34m(self, epoch, batch_idx, optimizer, optimizer_idx, optimizer_closure, on_tpu, using_native_amp, using_lbfgs)\u001b[0m\n\u001b[1;32m   1650\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1651\u001b[0m         \"\"\"\n\u001b[0;32m-> 1652\u001b[0;31m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mclosure\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moptimizer_closure\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1653\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1654\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0moptimizer_zero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepoch\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_idx\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mOptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer_idx\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pytorch_lightning/core/optimizer.py\u001b[0m in \u001b[0;36mstep\u001b[0;34m(self, closure, **kwargs)\u001b[0m\n\u001b[1;32m    162\u001b[0m         \u001b[0;32massert\u001b[0m \u001b[0mtrainer\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    163\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mtrainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprofiler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprofile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprofiler_action\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 164\u001b[0;31m             \u001b[0mtrainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maccelerator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptimizer_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_optimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_optimizer_idx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mclosure\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pytorch_lightning/accelerators/accelerator.py\u001b[0m in \u001b[0;36moptimizer_step\u001b[0;34m(self, optimizer, opt_idx, closure, model, **kwargs)\u001b[0m\n\u001b[1;32m    334\u001b[0m         \"\"\"\n\u001b[1;32m    335\u001b[0m         \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlightning_module\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 336\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprecision_plugin\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptimizer_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mopt_idx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mclosure\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    337\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    338\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0moptimizer_zero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcurrent_epoch\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_idx\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mOptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mopt_idx\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pytorch_lightning/plugins/precision/native_amp.py\u001b[0m in \u001b[0;36moptimizer_step\u001b[0;34m(self, model, optimizer, optimizer_idx, closure, **kwargs)\u001b[0m\n\u001b[1;32m     91\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpl\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mLightningModule\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautomatic_optimization\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mskipped_backward\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     92\u001b[0m             \u001b[0;31m# note: the scaler will skip the `optimizer.step` if nonfinite gradients are found\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 93\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mscaler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     94\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mscaler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     95\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/cuda/amp/grad_scaler.py\u001b[0m in \u001b[0;36mstep\u001b[0;34m(self, optimizer, *args, **kwargs)\u001b[0m\n\u001b[1;32m    334\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munscale_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    335\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 336\u001b[0;31m         \u001b[0;32massert\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moptimizer_state\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"found_inf_per_device\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"No inf checks were recorded for this optimizer.\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    337\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    338\u001b[0m         \u001b[0mretval\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_maybe_opt_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer_state\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAssertionError\u001b[0m: No inf checks were recorded for this optimizer."
     ]
    }
   ],
   "source": [
    "from pytorch_lightning import loggers as pl_loggers\n",
    "from pytorch_lightning.callbacks import EarlyStopping, ModelCheckpoint\n",
    "\n",
    "torch.manual_seed(123)\n",
    "\n",
    "tb_logger = pl_loggers.TensorBoardLogger('/content/drive/MyDrive/Colab Notebooks/Multimodal/tb_logs/')\n",
    "\n",
    "trainer = pl.Trainer(\n",
    "    min_epochs=1,\n",
    "    max_epochs=5,\n",
    "    gpus=1,\n",
    "    precision=16,\n",
    "    val_check_interval=0.5,\n",
    "    # check_val_every_n_epoch=1,\n",
    "    callbacks=[\n",
    "      ModelCheckpoint(\n",
    "          dirpath='/content/drive/MyDrive/Colab Notebooks/Multimodal/ckpt',\n",
    "          save_top_k=3,\n",
    "          monitor='f1/val',\n",
    "      ), \n",
    "      EarlyStopping('f1/val', patience=5)\n",
    "    ],\n",
    "    fast_dev_run=False,\n",
    "    logger=tb_logger\n",
    ")\n",
    "\n",
    "dm.setup(stage=\"fit\")\n",
    "trainer.fit(multi, dm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "8YFA9dl3Lkw2"
   },
   "outputs": [],
   "source": [
    "# multi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "sWmHn9ggNLfQ"
   },
   "outputs": [],
   "source": [
    "# from sklearn.metrics import roc_auc_score, classification_report, accuracy_score\n",
    "\n",
    "\n",
    "# class Concatmodal(nn.Module):\n",
    "#   def __init__(self, lr_multi, lr_classifier):\n",
    "#     super(Concatmodal,self).__init__()\n",
    "#     self.loss_module = nn.CrossEntropyLoss()\n",
    "#     self.BERT = Bert_Lstm()\n",
    "#     self.incept = Inception()\n",
    "#     self.dense = nn.Linear(256,256)\n",
    "#     self.cl = nn.Linear(256,3)\n",
    "#     self.bpe_path = \"/content/drive/MyDrive/NLP/PhoBERT_base_transformers/bpe.codes\"\n",
    "#     self.vocab_path = \"/content/drive/MyDrive/NLP/PhoBERT_base_transformers/dict.txt\"\n",
    "#     self.lr_multi = lr_multi\n",
    "#     self.lr_classifier = lr_classifier\n",
    "  \n",
    "#   def forward(self, image, texts, labels=None):\n",
    "#     inputs = tokenize_data(texts, self.vocab_path, self.bpe_path)\n",
    "#     # b_labels = None\n",
    "#     # b_labels = labels.to(self.device)\n",
    "\n",
    "#     for key in inputs:\n",
    "#         inputs[key] = inputs[key].to(self.device)\n",
    "#     image = image.to(self.device)\n",
    "\n",
    "#     fea1 = self.BERT(inputs)\n",
    "#     fea2 = self.incept(image)\n",
    "#     cat = torch.cat((fea1, fea2), 1)\n",
    "#     out = self.cl(cat)\n",
    "\n",
    "#     return out\n",
    "  \n",
    "\n",
    "# class Multimodal(pl.LightningModule):\n",
    "#   def __init__(self, lr_multi, lr_classifier):\n",
    "#     super(Multimodal,self).__init__()\n",
    "#     # self.loss_module = nn.CrossEntropyLoss()\n",
    "#     # self.BERT = Bert_Lstm()\n",
    "#     # self.incept = Inception()\n",
    "#     # self.dense = nn.Linear(256,256)\n",
    "#     # self.cl = nn.Linear(256,3)\n",
    "#     # self.bpe_path = \"/content/drive/MyDrive/NLP/PhoBERT_base_transformers/bpe.codes\"\n",
    "#     # self.vocab_path = \"/content/drive/MyDrive/NLP/PhoBERT_base_transformers/dict.txt\"\n",
    "#     self.lr_multi = lr_multi\n",
    "#     self.lr_classifier = lr_classifier\n",
    "\n",
    "#     self.model = self._build_model()\n",
    "#     # self.trainer_params = self._get_trainer_params()\n",
    "\n",
    "#   def forward(self, image, texts, labels=None):\n",
    "#     # inputs = tokenize_data(texts, self.vocab_path, self.bpe_path)\n",
    "#     # # b_labels = None\n",
    "#     # # b_labels = labels.to(self.device)\n",
    "\n",
    "#     # for key in inputs:\n",
    "#     #     inputs[key] = inputs[key].to(self.device)\n",
    "#     # image = image.to(self.device)\n",
    "\n",
    "#     # fea1 = self.BERT(inputs)\n",
    "#     # fea2 = self.incept(image)\n",
    "#     # cat = torch.cat((fea1, fea2), 1)\n",
    "#     # out = self.model(cat)\n",
    "\n",
    "#     return self.model(image, texts)\n",
    "  \n",
    "#   def _build_model(self):\n",
    "#     return Concatmodal(self.lr_multi, self.lr_classifier)\n",
    "  \n",
    "#   def configure_optimizers(self):\n",
    "#     # roberta_params = self.BERT.parameters()\n",
    "#     # inception_params = self.incept.parameters()\n",
    "#     # dense_params = self.dense.parameters()\n",
    "#     # classifier_params = self.cl.parameters()\n",
    "\n",
    "#     # for n,p in classifier_params:\n",
    "#     #   print(n,p)\n",
    "\n",
    "#     # grouped_params = [\n",
    "#     #     {\"params\": roberta_params, \"lr\": self.lr_multi},\n",
    "#     #     {\"params\": inception_params, \"lr\": self.lr_multi},\n",
    "#     #     {\"params\": dense_params, \"lr\": self.lr_multi},\n",
    "#     #     {\"params\": classifier_params, \"lr\": self.lr_classifier}\n",
    "#     # ]\n",
    "#     # optimizer = torch.optim.AdamW(\n",
    "#     #     grouped_params\n",
    "#     # )\n",
    "#     optimizer = torch.optim.AdamW(\n",
    "#               self.model.parameters(), \n",
    "#               lr = 0.001\n",
    "#           )\n",
    "\n",
    "#     scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=1000, gamma=0.98)\n",
    "  \n",
    "\n",
    "\n",
    "#   def training_step(self, batch, batch_idx):\n",
    "#     texts, images, labels = batch\n",
    "#     preds = self(images, texts, labels)\n",
    "\n",
    "#     loss = self.loss_module(preds, labels)\n",
    "\n",
    "#     print(\"train_loss\", loss)\n",
    "#     return loss\n",
    "  \n",
    "#   def validation_step(self, batch, batch_idx):\n",
    "#     texts, images, labels = batch\n",
    "#     logits = self(images, texts, labels)\n",
    "\n",
    "#     loss = self.loss_module(logits, labels)\n",
    "#     output_scores = torch.softmax(logits, dim=-1)\n",
    "\n",
    "#     return loss, output_scores, labels\n",
    "\n",
    "#   def validation_epoch_end(self, validation_step_outputs):\n",
    "#     val_preds = torch.tensor([], device=self.device)\n",
    "#     val_scores = torch.tensor([], device=self.device)\n",
    "#     val_labels = torch.tensor([], device=self.device)\n",
    "#     val_loss = 0\n",
    "#     total_item = 0\n",
    "\n",
    "#     for idx, item in enumerate(validation_step_outputs):\n",
    "#         loss, output_scores, labels = item\n",
    "\n",
    "#         predictions = torch.argmax(output_scores, dim=-1)\n",
    "#         val_preds = torch.cat((val_preds, predictions), dim=0)\n",
    "#         val_scores = torch.cat((val_scores, output_scores[:, 1]), dim=0)\n",
    "#         val_labels = torch.cat((val_labels, labels), dim=0)\n",
    "\n",
    "#         val_loss += loss\n",
    "#         total_item += 1\n",
    "\n",
    "#     # print(\"VAL PREDS\", val_preds.shape)\n",
    "#     # print(\"VAL SCORES\", val_scores.shape)\n",
    "#     # print(\"VAL LABELS\", val_labels.shape)\n",
    "#     val_preds = val_preds.cpu().numpy()\n",
    "#     val_scores = val_scores.cpu().numpy()\n",
    "#     val_labels = val_labels.cpu().numpy()\n",
    "\n",
    "#     # reports = classification_report(val_labels, val_preds, output_dict=True)\n",
    "#     print(\"VAL PREDS\", val_preds)\n",
    "#     print(\"VAL LABELS\", val_labels)\n",
    "#     print(\"VAL SCORES\", val_scores)\n",
    "#     # try:\n",
    "#     #     auc = roc_auc_score(val_labels, val_scores)\n",
    "#     # except Exception as e:\n",
    "#     #     print(e)\n",
    "#     #     print(\"Cannot calculate AUC. Default to 0\")\n",
    "#     #     auc = 0\n",
    "#     accuracy = accuracy_score(val_labels, val_preds)\n",
    "\n",
    "#     print(classification_report(val_labels, val_preds))\n",
    "\n",
    "#     self.log(\"loss/val\", val_loss)\n",
    "#     # self.log(\"auc/val\", auc)\n",
    "#     self.log(\"accuracy/val\", accuracy)\n",
    "#     # self.log(\"precision/val\", reports[\"weighted avg\"][\"precision\"])\n",
    "#     # self.log(\"recall/val\", reports[\"weighted avg\"][\"recall\"])\n",
    "#     # self.log(\"f1/val\", reports[\"weighted avg\"][\"f1-score\"])\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [
    "hBUVnSf-_ht0"
   ],
   "name": "Multimodal.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "02eca356512546b1954a563bc988d6b5": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "046f564c5ccc4b9f9c866a6a57b900bf": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "08e56fb9c8c74262aa75994d2766136d": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "0cfbf06467a04d4e9f97d8fbbb9328b7": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_345e16e44bad4ccf83b61099383e8b71",
      "placeholder": "​",
      "style": "IPY_MODEL_69e00ff1b849487f9e10000b486a766c",
      "value": "Downloading: 100%"
     }
    },
    "1250687b6c4e48e0a4b79de74f294235": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_0cfbf06467a04d4e9f97d8fbbb9328b7",
       "IPY_MODEL_16736771e702494cb52be951b3df47bc",
       "IPY_MODEL_df155493042740d9b96ee0343b9f03d4"
      ],
      "layout": "IPY_MODEL_70da72f327b64a8a8d2574848ddaa5c0"
     }
    },
    "16559060dd074447bf5e27134b4870ae": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": "inline-flex",
      "flex": null,
      "flex_flow": "row wrap",
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": "100%"
     }
    },
    "16736771e702494cb52be951b3df47bc": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_98128e9bcb0b4cf4a949d51f299b3bde",
      "max": 895321,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_302b8bb010ec40908960c26742dd4df3",
      "value": 895321
     }
    },
    "1bb3780855494b029edc7dc564fcc51e": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "1e21006c0b9740ecb6f427f6a49bf409": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "1f0e5ebe496b45ec97082f2bb666a82e": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": "2",
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "2132d76e642e4cbaaabf71e032d0fb8b": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "2889253c1e7d4df1b02db1773a304e26": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_876c019d15f04031af2f5735ed37a038",
       "IPY_MODEL_800ec21aa4674fce95f33685c2033b11",
       "IPY_MODEL_6b4ab9aa406a4071b5e888b0cd25335c"
      ],
      "layout": "IPY_MODEL_82995c53c15f4166a2641b2560faa49d"
     }
    },
    "2d90f769f9a748899fd536bc215e0902": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_1f0e5ebe496b45ec97082f2bb666a82e",
      "max": 2,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_2132d76e642e4cbaaabf71e032d0fb8b",
      "value": 2
     }
    },
    "2f8400d957d4428b86f73a07c12a69ff": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_b375fdd33a8f4fad9f7dcc33f24f3f7b",
       "IPY_MODEL_562df45b9b7e42799e8323f7db1eaf18",
       "IPY_MODEL_624a86d43249405388f1286063cebae4"
      ],
      "layout": "IPY_MODEL_c9918922671546a88e3b2ee509394df3"
     }
    },
    "302b8bb010ec40908960c26742dd4df3": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "31d14a5ec28640d8acf64959fb4a5aa8": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": "2",
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "345e16e44bad4ccf83b61099383e8b71": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "3d8af058291c4c7393bb04f2f1ec2622": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "3f3889c16b134b408de36adb46505c40": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "460e2a8033f04d4fbd9109254d2dfb73": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_93b193607ebd4e758cfc04a19b9dd2d6",
      "placeholder": "​",
      "style": "IPY_MODEL_b860941628094b5c8720672f5ebb3f3d",
      "value": " 0/540 [00:00&lt;?, ?it/s]"
     }
    },
    "49da75829cbd4350a64bfcdc04576ac1": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "4bea41d4adb249d5afc2cbd40dc3210d": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "562df45b9b7e42799e8323f7db1eaf18": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_49da75829cbd4350a64bfcdc04576ac1",
      "max": 557,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_4bea41d4adb249d5afc2cbd40dc3210d",
      "value": 557
     }
    },
    "624a86d43249405388f1286063cebae4": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_08e56fb9c8c74262aa75994d2766136d",
      "placeholder": "​",
      "style": "IPY_MODEL_ca0da54234e9446db60b5331fa076d81",
      "value": " 557/557 [00:00&lt;00:00, 5.30kB/s]"
     }
    },
    "664ccebe5b894806a2c84d633e107f38": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "69e00ff1b849487f9e10000b486a766c": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "6b4ab9aa406a4071b5e888b0cd25335c": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_ad4a2e90f75d49f289b127d43acca1df",
      "placeholder": "​",
      "style": "IPY_MODEL_d63ef1d399ab42698b667b2938d7e398",
      "value": " 1.08M/1.08M [00:00&lt;00:00, 3.02MB/s]"
     }
    },
    "70da72f327b64a8a8d2574848ddaa5c0": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "733c53bc95c0490982e6473ad1d55418": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "7939b8ff88a940028fb77f2bd715db87": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "800ec21aa4674fce95f33685c2033b11": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_3d8af058291c4c7393bb04f2f1ec2622",
      "max": 1135173,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_e4cb55de261c4416b945fdb72232c12d",
      "value": 1135173
     }
    },
    "81d26020a9784869b593de58ca922404": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "82995c53c15f4166a2641b2560faa49d": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "83e93a1ba3f24f96ac5ea9107e160a9e": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_7939b8ff88a940028fb77f2bd715db87",
      "placeholder": "​",
      "style": "IPY_MODEL_02eca356512546b1954a563bc988d6b5",
      "value": " 2/2 [00:03&lt;00:00,  1.54s/it]"
     }
    },
    "876c019d15f04031af2f5735ed37a038": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_1e21006c0b9740ecb6f427f6a49bf409",
      "placeholder": "​",
      "style": "IPY_MODEL_733c53bc95c0490982e6473ad1d55418",
      "value": "Downloading: 100%"
     }
    },
    "8d8bf547e78f461da79bb4e3e8449335": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_81d26020a9784869b593de58ca922404",
      "placeholder": "​",
      "style": "IPY_MODEL_664ccebe5b894806a2c84d633e107f38",
      "value": "Validation sanity check: 100%"
     }
    },
    "93b193607ebd4e758cfc04a19b9dd2d6": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "98128e9bcb0b4cf4a949d51f299b3bde": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "9aad7677f25b4c85abc0e6edb7b9212d": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_e47ffdaa9228458182c11c9ba945e33d",
       "IPY_MODEL_c4c4bae04bad46dea96a97dc5a7d8d0d",
       "IPY_MODEL_460e2a8033f04d4fbd9109254d2dfb73"
      ],
      "layout": "IPY_MODEL_fff7e36afcae4a3a81e5b40705c21519"
     }
    },
    "a93f05a257d5421eb2c72efcdac8fcd3": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_8d8bf547e78f461da79bb4e3e8449335",
       "IPY_MODEL_2d90f769f9a748899fd536bc215e0902",
       "IPY_MODEL_83e93a1ba3f24f96ac5ea9107e160a9e"
      ],
      "layout": "IPY_MODEL_16559060dd074447bf5e27134b4870ae"
     }
    },
    "ad4a2e90f75d49f289b127d43acca1df": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "b375fdd33a8f4fad9f7dcc33f24f3f7b": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_1bb3780855494b029edc7dc564fcc51e",
      "placeholder": "​",
      "style": "IPY_MODEL_046f564c5ccc4b9f9c866a6a57b900bf",
      "value": "Downloading: 100%"
     }
    },
    "b860941628094b5c8720672f5ebb3f3d": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "ba20e1bc66e94c829e2a46dd5d6a3483": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "bb16f7ad93f34d12babc320869e1413b": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "c4c4bae04bad46dea96a97dc5a7d8d0d": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_31d14a5ec28640d8acf64959fb4a5aa8",
      "max": 540,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_ba20e1bc66e94c829e2a46dd5d6a3483",
      "value": 0
     }
    },
    "c9918922671546a88e3b2ee509394df3": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "ca0da54234e9446db60b5331fa076d81": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "d63ef1d399ab42698b667b2938d7e398": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "db463b1c3ae44d6980bec291e144523c": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "df155493042740d9b96ee0343b9f03d4": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_3f3889c16b134b408de36adb46505c40",
      "placeholder": "​",
      "style": "IPY_MODEL_db463b1c3ae44d6980bec291e144523c",
      "value": " 874k/874k [00:00&lt;00:00, 1.31MB/s]"
     }
    },
    "e47ffdaa9228458182c11c9ba945e33d": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_bb16f7ad93f34d12babc320869e1413b",
      "placeholder": "​",
      "style": "IPY_MODEL_f7947613432143f98f2f714598d25dfd",
      "value": "Epoch 0:   0%"
     }
    },
    "e4cb55de261c4416b945fdb72232c12d": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "f7947613432143f98f2f714598d25dfd": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "fff7e36afcae4a3a81e5b40705c21519": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": "inline-flex",
      "flex": null,
      "flex_flow": "row wrap",
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": "100%"
     }
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
